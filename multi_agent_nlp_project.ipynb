{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb15c58",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒä¸ä¾èµ–æ£€æŸ¥\n",
    "\n",
    "å¦‚æœä½ è¿˜æ²¡æœ‰æŒ‰ç…§ `requirements.txt` å®‰è£…ä¾èµ–ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ï¼ˆå¯æŒ‰éœ€æ‰§è¡Œä¸€æ¬¡ï¼‰ï¼š\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "åœ¨ Notebook ä¸­ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼š\n",
    "\n",
    "```python\n",
    "# %pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://api.chatanywhere.tech/v1\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "print(\"ğŸ“¦ ç¯å¢ƒå˜é‡æ£€æŸ¥ï¼š\")\n",
    "print(\"  OPENAI_API_KEY:\", \"å·²é…ç½®\" if OPENAI_API_KEY else \"æœªé…ç½® (å°†ä½¿ç”¨ DummyLLM)\")\n",
    "print(\"  SERPAPI_API_KEY:\", \"å·²é…ç½®\" if SERPAPI_API_KEY else \"æœªé…ç½® (æœç´¢å·¥å…·å°†ä¸ºå ä½å®ç°)\")\n",
    "print(\"  OPENAI_BASE_URL:\", OPENAI_BASE_URL)\n",
    "print(\"  LLM_MODEL:\", LLM_MODEL)\n",
    "\n",
    "IS_DEEPSEEK = \"deepseek.com\" in (OPENAI_BASE_URL or \"\").lower()\n",
    "if IS_DEEPSEEK:\n",
    "    lm_lower = (LLM_MODEL or \"\").lower()\n",
    "    if lm_lower not in (\"deepseek-chat\", \"deepseek-reasoner\"):\n",
    "        new_model = \"deepseek-reasoner\" if (\"reason\" in lm_lower or \"think\" in lm_lower) else \"deepseek-chat\"\n",
    "        print(f\"â„¹ï¸ æ£€æµ‹åˆ° DeepSeek æ¥å£ï¼Œè‡ªåŠ¨å°†æ¨¡å‹å '{LLM_MODEL}' è§„èŒƒä¸º '{new_model}'ã€‚\")\n",
    "        LLM_MODEL = new_model\n"
   ],
   "id": "f6325852466c1fee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. LLM åˆå§‹åŒ–ï¼ˆä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰\n",
    "\n",
    "è¿™é‡Œå¤ç”¨è„šæœ¬ä¸­çš„ä¸‰å±‚å›é€€ç­–ç•¥ï¼š\n",
    "1. é¦–é€‰ `langchain_openai.ChatOpenAI`ã€‚\n",
    "2. å¤±è´¥æ—¶é€€åˆ°ç®€æ˜“ HTTP å®¢æˆ·ç«¯ `HTTPFallbackChat`ï¼ˆç›´æ¥è°ƒç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰ã€‚\n",
    "3. å†å¤±è´¥ä½¿ç”¨ `DummyLLM`ï¼Œåªè¾“å‡ºå ä½æ–‡æœ¬ï¼Œæ–¹ä¾¿åœ¨æ—  Key æƒ…å†µä¸‹è°ƒè¯•æµç¨‹ã€‚\n"
   ],
   "id": "31941c5b40b245fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "class DummyLLM:\n",
    "    \"\"\"ç¼ºå¤± API Key æ—¶ä½¿ç”¨çš„å ä½ LLMï¼Œå®ç° .invoke æ¥å£ã€‚\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = \"dummy-llm\"\n",
    "    def invoke(self, prompt):\n",
    "        if isinstance(prompt, dict):\n",
    "            return f\"[DummyLLM response for keys: {list(prompt.keys())}]\"\n",
    "        return \"[DummyLLM generic response]\"\n",
    "    def __or__(self, other):  # å…¼å®¹é“¾å¼è°ƒç”¨\n",
    "        return other\n",
    "\n",
    "\n",
    "class HTTPFallbackChat:\n",
    "    \"\"\"ç›´æ¥è°ƒç”¨ OpenAI å…¼å®¹æ¥å£çš„è½»é‡å®¢æˆ·ç«¯ï¼Œå®ç° .invoke(dict|str)ã€‚\"\"\"\n",
    "    def __init__(self, base_url: str, api_key: str, model: str, timeout: float = 30.0):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.timeout = timeout\n",
    "        if self.base_url.endswith('/v1'):\n",
    "            self.endpoint = f\"{self.base_url}/chat/completions\"\n",
    "        else:\n",
    "            self.endpoint = f\"{self.base_url}/v1/chat/completions\"\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        if isinstance(prompt, dict):\n",
    "            user_content = '\\n'.join(f\"{k}: {v}\" for k, v in prompt.items())\n",
    "        else:\n",
    "            user_content = str(prompt)\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an academic writing optimization assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(self.endpoint, headers=headers, json=payload, timeout=self.timeout)\n",
    "            if resp.status_code != 200:\n",
    "                return f\"[HTTPFallbackChat Error {resp.status_code}: {resp.text[:200]}]\"\n",
    "            data = resp.json()\n",
    "            return data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"[No content]\")\n",
    "        except Exception as e:\n",
    "            return f\"[HTTPFallbackChat Exception: {e}]\"\n",
    "\n",
    "    def __or__(self, other):\n",
    "        return other\n",
    "\n",
    "\n",
    "def init_llm():\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"âš ï¸ OPENAI_API_KEY æœªé…ç½®ï¼Œå°†ä½¿ç”¨ DummyLLMï¼ˆä»…ç”¨äºæµç¨‹è°ƒè¯•ï¼‰ã€‚\")\n",
    "        return DummyLLM()\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=0,\n",
    "            api_key=(lambda: OPENAI_API_KEY),\n",
    "            base_url=OPENAI_BASE_URL,\n",
    "        )\n",
    "        print(f\"âœ… ä¸» LLM å·²é€šè¿‡ ChatOpenAI åˆå§‹åŒ–: {LLM_MODEL} @ {OPENAI_BASE_URL}\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ChatOpenAI åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯• HTTPFallbackChat...\")\n",
    "        try:\n",
    "            fallback_llm = HTTPFallbackChat(OPENAI_BASE_URL, OPENAI_API_KEY, LLM_MODEL)\n",
    "            probe = fallback_llm.invoke(\"probe\")\n",
    "            if probe.startswith(\"[HTTPFallbackChat Error\") or probe.startswith(\"[HTTPFallbackChat Exception\"):\n",
    "                print(\"âš ï¸ HTTPFallbackChat æ¢æµ‹å¤±è´¥ï¼Œå›é€€åˆ° DummyLLMã€‚\")\n",
    "                return DummyLLM()\n",
    "            print(f\"âœ… HTTPFallbackChat å·²å°±ç»ª: {LLM_MODEL} @ {OPENAI_BASE_URL}\")\n",
    "            return fallback_llm\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ HTTPFallbackChat åˆå§‹åŒ–å¤±è´¥: {e2}ï¼Œå›é€€åˆ° DummyLLMã€‚\")\n",
    "            return DummyLLM()\n",
    "\n",
    "\n",
    "llm = init_llm()\n"
   ],
   "id": "a58428c9d72d29d"
  },
  {
   "cell_type": "markdown",
   "id": "2fd053ba",
   "metadata": {},
   "source": [
    "## 3. å·¥å…·å±‚ï¼šæœç´¢ / Python REPL / æ–‡ä»¶è¯»å†™\n",
    "\n",
    "è¿™é‡Œä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼š\n",
    "- å¦‚æœæ²¡æœ‰ SerpAPI Keyï¼Œåˆ™æœç´¢å·¥å…·è¿”å›å ä½æç¤ºï¼Œä½†æ¥å£ä»ç„¶å¯ç”¨ã€‚\n",
    "- æä¾› Python REPL å·¥å…·æ‰§è¡Œç®€å•ä»£ç ç‰‡æ®µã€‚\n",
    "- æä¾›æœ€å°çš„æ–‡æœ¬æ–‡ä»¶è¯»å†™å·¥å…·ï¼Œæ–¹ä¾¿åœ¨ Notebook ä¸­å¿«é€Ÿè¯•éªŒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8d1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½ 4 ä¸ªå·¥å…·ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "TOOLS = []\n",
    "\n",
    "if SERPAPI_API_KEY:\n",
    "    search_wrapper = SerpAPIWrapper(search_engine=\"google\", serpapi_api_key=SERPAPI_API_KEY)\n",
    "    search_tool = Tool(\n",
    "        name=\"ç½‘ç»œæœç´¢\",\n",
    "        func=search_wrapper.run,\n",
    "        description=\"å®æ—¶ä¿¡æ¯æŸ¥è¯¢ï¼šè¾“å…¥æœç´¢å…³é”®è¯\"\n",
    "    )\n",
    "    TOOLS.append(search_tool)\n",
    "else:\n",
    "    def _search_stub(q: str) -> str:\n",
    "        return f\"[SerpAPI æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢: {q}]\"\n",
    "    search_tool = Tool(name=\"ç½‘ç»œæœç´¢\", func=_search_stub, description=\"SerpAPI æœªé…ç½®ï¼Œå ä½æœç´¢å·¥å…·\")\n",
    "    TOOLS.append(search_tool)\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl_tool = Tool(\n",
    "    name=\"Python REPL\",\n",
    "    func=python_repl.run,\n",
    "    description=\"æ‰§è¡Œæ ¼å¼æ­£ç¡®çš„ Python ä»£ç \"\n",
    ")\n",
    "\n",
    "read_file_tool = Tool(\n",
    "    name=\"è¯»å–æ–‡ä»¶\",\n",
    "    func=lambda fn: open(fn, \"r\", encoding=\"utf-8\").read(),\n",
    "    description=\"è¯»å–æŒ‡å®šæ–‡æœ¬æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œè¾“å…¥ä¸ºæ–‡ä»¶è·¯å¾„\"\n",
    ")\n",
    "\n",
    "write_file_tool = Tool(\n",
    "    name=\"å†™å…¥æ–‡ä»¶\",\n",
    "    func=lambda arg: (\n",
    "        (lambda filename, content: (open(filename, \"w\", encoding=\"utf-8\").write(content), \"å†™å…¥å®Œæˆ\")[1])\n",
    "    )(*arg.split(\",\", 1)),\n",
    "    description=\"å†™å…¥æ–‡ä»¶å†…å®¹ã€‚è¾“å…¥æ ¼å¼: æ–‡ä»¶å,å†…å®¹ï¼ˆè‹±æ–‡é€—å·åˆ†éš”ï¼‰\"\n",
    ")\n",
    "\n",
    "TOOLS.extend([python_repl_tool, read_file_tool, write_file_tool])\n",
    "\n",
    "print(f\"ğŸ”§ å·²åŠ è½½å·¥å…·æ•°é‡: {len(TOOLS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374ef3d",
   "metadata": {},
   "source": [
    "## 4. å‘é‡è®°å¿†å±‚ï¼šFAISS æˆ–ç®€æ˜“å†…å­˜æ£€ç´¢\n",
    "\n",
    "ä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼š\n",
    "- ä¼˜å…ˆä½¿ç”¨ FAISS + OpenAIEmbeddingsï¼ˆæˆ– DummyEmbeddingsï¼‰ã€‚\n",
    "- å¦‚æœ FAISS/ä¾èµ–ç¼ºå¤±ï¼Œåˆ™é€€å›åˆ°ä¸€ä¸ªåŸºäº token é‡å åº¦çš„ç®€æ˜“ `SimpleVectorStore`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbd82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‘é‡æ•°æ®åº“è®°å¿†æ¨¡å—å·²åˆå§‹åŒ–ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "EMBED_DIM = 1536\n",
    "USE_FAISS = True\n",
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "    import faiss\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_core.embeddings import Embeddings as LCEmbeddings\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ å‘é‡å­˜å‚¨ä¾èµ–ç¼ºå¤±: {e}ï¼Œä½¿ç”¨ç®€æ˜“å†…å­˜æ£€ç´¢æ›¿ä»£ FAISSã€‚\")\n",
    "    USE_FAISS = False\n",
    "    FAISS = None  # type: ignore\n",
    "    InMemoryDocstore = None  # type: ignore\n",
    "    faiss = None  # type: ignore\n",
    "    try:\n",
    "        from langchain_core.documents import Document\n",
    "    except Exception:\n",
    "        class Document:  # æœ€å°å ä½å®ç°\n",
    "            def __init__(self, page_content: str, metadata: Optional[Dict] = None):\n",
    "                self.page_content = page_content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "    class LCEmbeddings:  # type: ignore\n",
    "        _dim = EMBED_DIM\n",
    "        def embed_query(self, x: str):\n",
    "            return [0.0] * self._dim\n",
    "        def embed_documents(self, xs: List[str]):\n",
    "            return [[0.0] * self._dim for _ in xs]\n",
    "\n",
    "\n",
    "EMBED_MODEL_NAME = os.getenv(\"EMBED_MODEL_NAME\", \"text-embedding-3-small\")\n",
    "\n",
    "class DummyEmbeddings:\n",
    "    def embed_query(self, t: str):\n",
    "        return [0.0] * EMBED_DIM\n",
    "    def embed_documents(self, docs: List[str]):\n",
    "        return [[0.0] * EMBED_DIM for _ in docs]\n",
    "    def __call__(self, t: str):\n",
    "        return self.embed_query(t)\n",
    "\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    try:\n",
    "        if IS_DEEPSEEK:\n",
    "            print(\"â„¹ï¸ æ£€æµ‹åˆ° DeepSeek base_urlï¼Œè·³è¿‡ OpenAIEmbeddingsï¼Œä½¿ç”¨å ä½ embeddingã€‚\")\n",
    "            embeddings_model = DummyEmbeddings()\n",
    "        else:\n",
    "            from langchain_openai import OpenAIEmbeddings\n",
    "            embeddings_model = OpenAIEmbeddings(\n",
    "                model=EMBED_MODEL_NAME,\n",
    "                api_key=(lambda: OPENAI_API_KEY),\n",
    "                base_url=OPENAI_BASE_URL\n",
    "            )\n",
    "            print(f\"âœ… Embeddings æ¨¡å‹å·²åˆå§‹åŒ–: {EMBED_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Embeddings åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨ DummyEmbeddingsã€‚\")\n",
    "        embeddings_model = DummyEmbeddings()\n",
    "else:\n",
    "    embeddings_model = DummyEmbeddings()\n",
    "    print(\"âš ï¸ OPENAI_API_KEY ç¼ºå¤±ï¼ŒEmbeddings ä½¿ç”¨å ä½å‘é‡ã€‚\")\n",
    "\n",
    "_DEF_WORD_RE = re.compile(r\"[\\u4e00-\\u9fff]|[A-Za-z0-9_]+\")\n",
    "\n",
    "def _simple_tokenize(text: str) -> List[str]:\n",
    "    return _DEF_WORD_RE.findall(text or \"\")\n",
    "\n",
    "\n",
    "if USE_FAISS:\n",
    "    class EmbeddingAdapter(LCEmbeddings):\n",
    "        def __init__(self, base):\n",
    "            self.base = base\n",
    "        def embed_query(self, x: str) -> List[float]:\n",
    "            try:\n",
    "                return self.base.embed_query(x)\n",
    "            except Exception:\n",
    "                return [0.0] * EMBED_DIM\n",
    "        def embed_documents(self, xs: List[str]) -> List[List[float]]:\n",
    "            try:\n",
    "                return self.base.embed_documents(xs)\n",
    "            except Exception:\n",
    "                return [[0.0] * EMBED_DIM for _ in xs]\n",
    "\n",
    "    adapter = EmbeddingAdapter(embeddings_model)\n",
    "    index = faiss.IndexFlatL2(EMBED_DIM)\n",
    "    vectorstore = FAISS(adapter, index, InMemoryDocstore({}), {})\n",
    "    print(\"ğŸ§  å‘é‡æ•°æ®åº“(FAISS)å·²åˆå§‹åŒ–ã€‚\")\n",
    "else:\n",
    "    class SimpleVectorStore:\n",
    "        def __init__(self):\n",
    "            self.docs: List[Document] = []\n",
    "        def add_documents(self, docs: List[Document]):\n",
    "            self.docs.extend(docs)\n",
    "        def similarity_search(self, query: str, k: int = 3) -> List[Document]:\n",
    "            q_tokens = set(_simple_tokenize(query))\n",
    "            def score(doc: Document):\n",
    "                d_tokens = set(_simple_tokenize(doc.page_content))\n",
    "                if not q_tokens or not d_tokens:\n",
    "                    return 0.0\n",
    "                return len(q_tokens & d_tokens) / len(q_tokens | d_tokens)\n",
    "            ranked = sorted(self.docs, key=score, reverse=True)\n",
    "            return ranked[:k]\n",
    "\n",
    "    vectorstore = SimpleVectorStore()\n",
    "    print(\"ğŸ§  å‘é‡æ•°æ®åº“ç®€åŒ–ç‰ˆå·²åˆå§‹åŒ–ï¼ˆæ—  FAISSï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, vs, namespace: str = \"global\"):\n",
    "        self.vs = vs\n",
    "        self.namespace = namespace\n",
    "        self._counter = 0\n",
    "    def add_memory(self, text: str, metadata: Optional[Dict] = None):\n",
    "        try:\n",
    "            meta = metadata or {}\n",
    "            meta.update({\"namespace\": self.namespace, \"ts\": datetime.now().isoformat()})\n",
    "            doc = Document(page_content=text, metadata=meta)\n",
    "            self.vs.add_documents([doc])\n",
    "            self._counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ å†™å…¥è®°å¿†å¤±è´¥: {e}\")\n",
    "    def recall(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            res = self.vs.similarity_search(query, k=k)\n",
    "            return [d.page_content for d in res]\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–è®°å¿†å¤±è´¥: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e734ef4",
   "metadata": {},
   "source": [
    "## 5. åŒ Agent å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ç³»ç»Ÿï¼ˆä¸è„šæœ¬ç‰ˆæœ¬åŒæ­¥ï¼‰\n",
    "\n",
    "è¿™é‡Œç›´æ¥å¤ç”¨ `multi_agent_nlp_project.py` ä¸­çš„å®ç°ï¼Œåªä¿ç•™ä¸â€œåä½œä¼˜åŒ–â€å¼ºç›¸å…³çš„éƒ¨åˆ†ï¼Œæ–¹ä¾¿åœ¨ Notebook ä¸­äº¤äº’å¼è°ƒç”¨ã€‚\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import difflib\n",
    "import time\n",
    "\n",
    "\n",
    "class DualAgentAcademicSystem:\n",
    "    def __init__(self, llm, tools, vectorstore, enable_tools: bool = True, enable_memory: bool = True):\n",
    "        self.llm = llm\n",
    "        self.tools_enabled = enable_tools\n",
    "        self.memory_enabled = enable_memory\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.vectorstore = vectorstore\n",
    "        self.memory = MemoryManager(vectorstore) if enable_memory else None\n",
    "        self.collaboration_log: List[Dict] = []\n",
    "        self._setup_agents()\n",
    "\n",
    "    def _setup_agents(self):\n",
    "        self.agent_a_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "ä½ æ˜¯Agent A - å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ä¸“å®¶ã€‚\n",
    "è½®æ¬¡: ç¬¬{round_num}è½®\n",
    "ç”¨æˆ·éœ€æ±‚: {user_requirements}\n",
    "ä¸Šä¸€è½®è¯„åˆ†(è‹¥æœ‰): {last_scores}\n",
    "é•¿ç¨‹è®°å¿†æ£€ç´¢ç‰‡æ®µ:\n",
    "{memory_snippets}\n",
    "å·¥å…·è§‚å¯Ÿ:\n",
    "{tool_observations}\n",
    "å¾…ä¼˜åŒ–æ–‡æœ¬:\n",
    "{text_to_optimize}\n",
    "{previous_feedback}\n",
    "\n",
    "è¯·è¾“å‡ºï¼š\n",
    "**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\n",
    "[ä¼˜åŒ–åçš„å®Œæ•´æ–‡æœ¬]\n",
    "\n",
    "**ä¿®æ”¹è¯´æ˜ï¼š**\n",
    "[è¯´æ˜æœ¬è½®ä¿®æ”¹è¦ç‚¹ï¼Œå°¤å…¶é’ˆå¯¹è¯„å®¡æå‡ºçš„é«˜ä¼˜å…ˆçº§é—®é¢˜]\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.agent_b_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "ä½ æ˜¯Agent B - å­¦æœ¯è¯„å®¡ä¸å¯¹æŠ—è´¨è¯¢ä¸“å®¶ã€‚\n",
    "è½®æ¬¡: ç¬¬{round_num}è½®\n",
    "ç”¨æˆ·éœ€æ±‚: {user_requirements}\n",
    "ä¼˜åŒ–æ–‡æœ¬:\n",
    "{optimized_text}\n",
    "\n",
    "è¯·è¯„å®¡å¹¶è¾“å‡º(ä¸¥æ ¼åŒ…å«ä»¥ä¸‹æ¿å—ä¸æ•°å€¼)ï¼š\n",
    "**æœ¬è½®æ”¹è¿›è¯„ä»·ï¼š**\n",
    "[æ€»ä½“è¯„ä»·]\n",
    "\n",
    "**è¯„åˆ†(è¯·ä½¿ç”¨JSONæ ¼å¼)**\n",
    "{{\"quality\": <1-10>, \"rigor\": <1-10>, \"logic\": <1-10>, \"novelty\": <1-10>, \"priority_issues\": <æè¿°>}}\n",
    "\n",
    "**å‰©ä½™ä¸»è¦é—®é¢˜ï¼š**\n",
    "[...]\n",
    "\n",
    "**ä¸‹è½®é‡ç‚¹å»ºè®®ï¼š**\n",
    "1. [...]\n",
    "2. [...]\n",
    "\n",
    "**æ”¹è¿›ä¼˜å…ˆçº§ï¼š**\n",
    "[é«˜/ä¸­/ä½ åˆ†å±‚åˆ—å‡º]\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.agent_a_chain = self.agent_a_template | self.llm | StrOutputParser()\n",
    "        self.agent_b_chain = self.agent_b_template | self.llm | StrOutputParser()\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_section(text: str, start_token: str, end_token: str) -> str:\n",
    "        lines = text.split('\\n')\n",
    "        collecting = False\n",
    "        buf = []\n",
    "        for l in lines:\n",
    "            if start_token in l:\n",
    "                collecting = True\n",
    "                continue\n",
    "            if collecting and end_token in l:\n",
    "                break\n",
    "            if collecting:\n",
    "                buf.append(l)\n",
    "        return '\\n'.join(buf).strip()\n",
    "\n",
    "    def _compute_diff(self, prev: str, current: str) -> str:\n",
    "        if prev is None:\n",
    "            return '(é¦–è½®æ— diff)'\n",
    "        diff_lines = difflib.unified_diff(prev.splitlines(), current.splitlines(), lineterm='')\n",
    "        collected = []\n",
    "        for i, line in enumerate(diff_lines):\n",
    "            if i > 400:\n",
    "                collected.append('... <diff truncated>')\n",
    "                break\n",
    "            collected.append(line)\n",
    "        return '\\n'.join(collected) if collected else '(æ— å˜åŒ–)'\n",
    "\n",
    "    def _parse_scores(self, feedback: str) -> Dict[str, float]:\n",
    "        import json as _json, re as _re\n",
    "        m = _re.search(r'\\{\\s*\"quality\".*?\\}', feedback, flags=_re.S)\n",
    "        if not m:\n",
    "            return {}\n",
    "        blob = m.group(0)\n",
    "        try:\n",
    "            data = _json.loads(blob)\n",
    "            for k in [\"quality\", \"rigor\", \"logic\", \"novelty\"]:\n",
    "                if k in data:\n",
    "                    data[k] = float(data[k])\n",
    "            return data\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    def _plan_and_act(self, text: str, requirements: List[str]) -> str:\n",
    "        if not self.tools_enabled:\n",
    "            return \"(å·¥å…·å·²ç¦ç”¨)\"\n",
    "        observations = []\n",
    "        joined_req = ' '.join(requirements).lower()\n",
    "        if any(kw in joined_req for kw in [\"search\", \"æ£€ç´¢\", \"äº‹å®\", \"æœ€æ–°\", \"å¼•ç”¨\"]):\n",
    "            m = re.findall(r'\"([^\\\"]+)\"', text)\n",
    "            query = m[-1] if m else text.split('ã€‚')[-1].strip() or text\n",
    "            try:\n",
    "                obs = self.tools[\"ç½‘ç»œæœç´¢\"].run(query)\n",
    "            except Exception as e:\n",
    "                obs = f\"[æœç´¢å¼‚å¸¸: {e}]\"\n",
    "            observations.append(f\"æœç´¢[{query}] -> {obs[:300]}\")\n",
    "        return \"\\n\".join(observations) if observations else \"(æ— )\"\n",
    "\n",
    "    def collaborate(self, user_text: str, user_requirements: List[str], language: str = \"ä¸­æ–‡\", rounds: int = 3) -> Tuple[str, List[Dict]]:\n",
    "        self.collaboration_log = [{\"round\": 0, \"user_input\": user_text, \"requirements\": user_requirements, \"timestamp\": datetime.now().isoformat()}]\n",
    "        current_text = user_text\n",
    "        previous_feedback = \"\"\n",
    "        last_scores: Dict[str, float] = {}\n",
    "\n",
    "        if self.memory_enabled:\n",
    "            self.memory.add_memory(user_text, {\"type\": \"user_input\"})\n",
    "\n",
    "        for r in range(1, rounds + 1):\n",
    "            mem_snippets = []\n",
    "            if self.memory_enabled:\n",
    "                mem_snippets = self.memory.recall(current_text, k=3)\n",
    "            tool_obs = self._plan_and_act(current_text, user_requirements)\n",
    "\n",
    "            a_input = {\n",
    "                \"round_num\": r,\n",
    "                \"text_to_optimize\": current_text,\n",
    "                \"user_requirements\": ', '.join(user_requirements),\n",
    "                \"previous_feedback\": previous_feedback,\n",
    "                \"memory_snippets\": '\\n'.join(mem_snippets) if mem_snippets else \"(æ— )\",\n",
    "                \"tool_observations\": tool_obs,\n",
    "                \"last_scores\": last_scores if last_scores else \"(æ— )\"\n",
    "            }\n",
    "            a_resp = self.agent_a_chain.invoke(a_input)\n",
    "            optimized_text = self._extract_section(a_resp, \"**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\", \"**ä¿®æ”¹è¯´æ˜ï¼š**\") or current_text\n",
    "\n",
    "            b_input = {\n",
    "                \"round_num\": r,\n",
    "                \"optimized_text\": optimized_text,\n",
    "                \"user_requirements\": ', '.join(user_requirements)\n",
    "            }\n",
    "            b_resp = self.agent_b_chain.invoke(b_input)\n",
    "\n",
    "            last_scores = self._parse_scores(b_resp)\n",
    "            diff_str = self._compute_diff(current_text, optimized_text)\n",
    "\n",
    "            if self.memory_enabled:\n",
    "                self.memory.add_memory(optimized_text, {\"type\": \"optimized_text\", \"round\": r})\n",
    "                self.memory.add_memory(b_resp, {\"type\": \"feedback\", \"round\": r})\n",
    "\n",
    "            self.collaboration_log.append({\n",
    "                \"round\": r,\n",
    "                \"agent_a_response\": a_resp,\n",
    "                \"optimized_text\": optimized_text,\n",
    "                \"agent_b_feedback\": b_resp,\n",
    "                \"scores\": last_scores,\n",
    "                \"tool_observations\": tool_obs,\n",
    "                \"diff\": diff_str,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "            previous_feedback = b_resp\n",
    "            current_text = optimized_text\n",
    "            print(f\"âœ… Round {r} å®Œæˆ | è¯„åˆ†: {last_scores if last_scores else '{}'}\")\n",
    "            time.sleep(0.15)\n",
    "\n",
    "        return current_text, self.collaboration_log\n",
    "\n",
    "\n",
    "dual_agent_system = DualAgentAcademicSystem(llm, TOOLS, vectorstore)\n",
    "print(\"ğŸ¤– åŒAgentç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ˆNotebook ç‰ˆï¼‰\")\n"
   ],
   "id": "f815d1e6ec131a76"
  },
  {
   "cell_type": "markdown",
   "id": "79cce398",
   "metadata": {},
   "source": [
    "## 6. Notebook ä¸­çš„ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "ä¸‹é¢ç»™å‡ºå‡ ä¸ªå…¸å‹ç”¨æ³•ç¤ºä¾‹ï¼š\n",
    "- å•æ®µæ–‡æœ¬å¤šè½®å­¦æœ¯ä¼˜åŒ–ã€‚\n",
    "- è°ƒæ•´éœ€æ±‚ç»´åº¦ã€è½®æ¬¡æ•°ã€‚\n",
    "- è§‚å¯Ÿå¤šè½®åä½œæ—¥å¿—ç»“æ„ã€‚\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sample_text = \"è¿™æ˜¯ä¸€æ®µå…³äºå¤šæ™ºèƒ½ä½“åä½œè¿›è¡Œå­¦æœ¯å†™ä½œä¼˜åŒ–çš„åˆç¨¿ï¼Œè¡¨è¿°ç•¥æ˜¾å£è¯­åŒ–ï¼Œç»“æ„ä¹Ÿä¸å¤Ÿæ¸…æ™°ã€‚\"\n",
    "requirements = [\"å­¦æœ¯è¡¨è¾¾æå‡\", \"é€»è¾‘ç»“æ„ä¼˜åŒ–\"]\n",
    "\n",
    "final_text, log = dual_agent_system.collaborate(sample_text, requirements, rounds=2)\n",
    "\n",
    "print(\"\\nğŸ“Œ æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬:\\n\")\n",
    "print(final_text)\n",
    "\n",
    "print(\"\\nğŸ“œ åä½œæ—¥å¿—æ‘˜è¦ï¼ˆæœ€åä¸€è½®ï¼‰:\\n\")\n",
    "pprint(log[-1])\n"
   ],
   "id": "b023f3362946bf9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> ä½ å¯ä»¥åœ¨æœ¬ Notebook ä¸­æ ¹æ®éœ€è¦ç»§ç»­æ·»åŠ ï¼š\n",
    "> - æ•°æ®åˆæˆ / è’¸é¦å¯¹å‡†å¤‡çš„å®éªŒæ€§å•å…ƒæ ¼ï¼Œç›´æ¥è°ƒç”¨ `multi_agent_nlp_project.py` ä¸­çš„ CLIï¼›\n",
    "> - å°†é•¿æ–‡æœ¬æŒ‰æ®µæ‹†åˆ†åè°ƒç”¨ `dual_agent_system.collaborate` çš„ Notebook ç‰ˆæœ¬å°è£…ï¼›\n",
    "> - æˆ–è€…ç®€å•ä½œä¸ºè„šæœ¬è¡Œä¸ºçš„å¯è§†åŒ–ä¸è°ƒè¯•ç¯å¢ƒã€‚\n",
    "\n"
   ],
   "id": "17c21777615c5587"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
