{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c20c09",
   "metadata": {},
   "source": [
    "# å¤šæ™ºèƒ½ä½“ NLP é¡¹ç›®ï¼šæ¶æ„ä¸å®ç°\n",
    "\n",
    "æœ¬é¡¹ç›®æ—¨åœ¨æ¼”ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªé›†æˆäº†å¤§è„‘ã€è§„åˆ’ã€å·¥å…·å’Œè®°å¿†ç­‰ç»„ä»¶çš„å¤æ‚ NLP æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬è¿˜å°†æ¢ç´¢æ•°æ®åˆæˆä¸æ¨¡å‹è’¸é¦çš„æ¦‚å¿µã€‚\n",
    "\n",
    "**é¡¹ç›®ç»“æ„:**\n",
    "1.  **ç¯å¢ƒè®¾ç½®**: å®‰è£…å¿…è¦çš„ Python åº“ã€‚\n",
    "2.  **API å¯†é’¥é…ç½®**: å®‰å…¨åœ°åŠ è½½ API å¯†é’¥ã€‚\n",
    "3.  **æ™ºèƒ½ä½“æ¶æ„**:\n",
    "    *   **å¤§è„‘**: é›†æˆ GPT-4 LLMã€‚\n",
    "    *   **å·¥å…·**: å®šä¹‰ç½‘ç»œæœç´¢ã€ä»£ç æ‰§è¡Œå’Œæ–‡ä»¶æ“ä½œç­‰å·¥å…·ã€‚\n",
    "    *   **è®°å¿†**: ä½¿ç”¨ FAISS å‘é‡æ•°æ®åº“å®ç°é•¿æœŸè®°å¿†ã€‚\n",
    "    *   **è§„åˆ’**: æ„å»ºä¸€ä¸ª ReAct æ¡†æ¶çš„æ™ºèƒ½ä½“ã€‚\n",
    "4.  **æ•°æ®åˆæˆ**: æ¼”ç¤ºæ™ºèƒ½ä½“å¦‚ä½•æ ¹æ®è§’è‰²å’Œä»»åŠ¡ç”Ÿæˆå†…å®¹ã€‚\n",
    "5.  **æ•°æ®è’¸é¦**: å±•ç¤ºâ€œæ•™å¸ˆ-å­¦ç”Ÿâ€æ¨¡å‹å¦‚ä½•ååŒå·¥ä½œä»¥ä¼˜åŒ–è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227406cd",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®ä¸ä¾èµ–é¡¹å®‰è£…\n",
    "\n",
    "æ­¤éƒ¨åˆ†ä»£ç å°†ä½¿ç”¨ `pip` å®‰è£…æ‰€æœ‰å¿…éœ€çš„ Python åº“ã€‚ä¸ºäº†è¿è¡Œæ­¤é¡¹ç›®ï¼Œä½ éœ€è¦ `openai` (ä¸ GPT-4 äº¤äº’), `langchain` (æ„å»ºæ™ºèƒ½ä½“çš„ä¸»è¦æ¡†æ¶), `google-search-results` (ç”¨äºç½‘ç»œæœç´¢), `faiss-cpu` (å‘é‡æ•°æ®åº“), `tiktoken` (ç”¨äºæ–‡æœ¬åˆ†è¯) ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cab184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\programdata\\miniforge\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: langchain in d:\\programdata\\miniforge\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: langchain-openai in d:\\programdata\\miniforge\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: langchain-community in d:\\programdata\\miniforge\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-experimental in d:\\programdata\\miniforge\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: langchain-core in d:\\programdata\\miniforge\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: google-search-results in d:\\programdata\\miniforge\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: faiss-cpu in d:\\programdata\\miniforge\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: tiktoken in d:\\programdata\\miniforge\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\programdata\\miniforge\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\programdata\\miniforge\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programdata\\miniforge\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\miniforge\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\miniforge\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programdata\\miniforge\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\programdata\\miniforge\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\programdata\\miniforge\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\programdata\\miniforge\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-core) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programdata\\miniforge\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\programdata\\miniforge\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\programdata\\miniforge\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\programdata\\miniforge\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\programdata\\miniforge\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\programdata\\miniforge\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in d:\\programdata\\miniforge\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\miniforge\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniforge\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\programdata\\miniforge\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\programdata\\miniforge\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\miniforge\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai langchain langchain-openai langchain-community langchain-experimental langchain-core google-search-results faiss-cpu tiktoken python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb15c58",
   "metadata": {},
   "source": [
    "## 2. API å¯†é’¥ä¸é…ç½®\n",
    "\n",
    "**æå…¶é‡è¦**: ä¸ºäº†ä¿æŠ¤ä½ çš„ API å¯†é’¥ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡æ¥åŠ è½½å®ƒä»¬ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ã€‚\n",
    "\n",
    "**æ“ä½œæ­¥éª¤**:\n",
    "1.  åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º `.env` çš„æ–‡ä»¶ã€‚\n",
    "2.  å°†ä½ çš„ API å¯†é’¥æ·»åŠ åˆ° `.env` æ–‡ä»¶ä¸­ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n",
    "    ```\n",
    "    OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "    SERPAPI_API_KEY=\"YOUR_SERPAPI_API_KEY\"\n",
    "    ```\n",
    "3.  **ç¡®ä¿å°† `.env` æ–‡ä»¶æ·»åŠ åˆ°ä½ çš„ `.gitignore` æ–‡ä»¶ä¸­**ï¼Œä»¥é˜²æ­¢å®ƒè¢«æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç å°†ä½¿ç”¨ `python-dotenv` åº“æ¥åŠ è½½è¿™äº›å¯†é’¥ã€‚åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `\"API_KEY\"` ä½œä¸ºå ä½ç¬¦ï¼Œä½ **å¿…é¡»**åœ¨è¿è¡Œå‰æ›¿æ¢å®ƒä»¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1d67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” API å¯†é’¥é…ç½®æ£€æŸ¥:\n",
      "------------------------------\n",
      "âœ… OpenAI API å¯†é’¥æ ¼å¼æ­£ç¡®\n",
      "âœ… SerpAPI å¯†é’¥å·²è®¾ç½®\n",
      "------------------------------\n",
      "ğŸ‰ API å¯†é’¥é…ç½®å®Œæˆï¼å‡†å¤‡å¼€å§‹...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# è·å– API å¯†é’¥\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# è¯¦ç»†çš„é…ç½®æ£€æŸ¥\n",
    "print(\"ğŸ” API å¯†é’¥é…ç½®æ£€æŸ¥:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API å¯†é’¥ (OPENAI_API_KEY) æœªè®¾ç½®\")\n",
    "    print(\"   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=\\\"æ‚¨çš„å¯†é’¥\\\"\")\n",
    "elif openai_api_key.startswith(\"sk-\") and len(openai_api_key) > 20:\n",
    "    print(\"âœ… OpenAI API å¯†é’¥æ ¼å¼æ­£ç¡®\")\n",
    "else:\n",
    "    print(\"âš ï¸  OpenAI API å¯†é’¥æ ¼å¼å¯èƒ½ä¸æ­£ç¡®\")\n",
    "    print(\"   å¯†é’¥åº”è¯¥ä»¥ 'sk-' å¼€å¤´ä¸”é•¿åº¦è¾ƒé•¿\")\n",
    "\n",
    "if not serpapi_api_key:\n",
    "    print(\"âŒ SerpAPI å¯†é’¥ (SERPAPI_API_KEY) æœªè®¾ç½®\")\n",
    "    print(\"   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : SERPAPI_API_KEY=\\\"æ‚¨çš„å¯†é’¥\\\"\")\n",
    "elif len(serpapi_api_key) > 10:\n",
    "    print(\"âœ… SerpAPI å¯†é’¥å·²è®¾ç½®\")\n",
    "else:\n",
    "    print(\"âš ï¸  SerpAPI å¯†é’¥é•¿åº¦å¯èƒ½ä¸æ­£ç¡®\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if openai_api_key and serpapi_api_key:\n",
    "    if (openai_api_key.startswith(\"sk-\") and \n",
    "        openai_api_key != \"YOUR_OPENAI_API_KEY\" and \n",
    "        serpapi_api_key != \"YOUR_SERPAPI_API_KEY\"):\n",
    "        print(\"ğŸ‰ API å¯†é’¥é…ç½®å®Œæˆï¼å‡†å¤‡å¼€å§‹...\")\n",
    "    else:\n",
    "        print(\"âš ï¸  è¯·ç¡®ä¿ä½¿ç”¨çœŸå®çš„APIå¯†é’¥ï¼Œä¸æ˜¯å ä½ç¬¦\")\n",
    "else:\n",
    "    print(\"âŒ è¯·å…ˆé…ç½® API å¯†é’¥æ‰èƒ½ç»§ç»­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1327b27",
   "metadata": {},
   "source": [
    "## 3. å¤§è„‘ï¼šé›†æˆ LLM (GPT-4o-mini)\n",
    "\n",
    "æ™ºèƒ½ä½“çš„\"å¤§è„‘\"æ˜¯å…¶æ ¸å¿ƒå†³ç­–ç»„ä»¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ LangChain æ¡†æ¶æ¥å®ä¾‹åŒ–ä¸€ä¸ªå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿ç”¨ GPT-4o-miniã€‚è¿™ä¸ªæ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå…è´¹ç‰ˆæœ¬æä¾› 200æ¬¡/å¤© çš„è°ƒç”¨é¢åº¦ï¼Œéå¸¸é€‚åˆæ™ºèƒ½ä½“å¼€å‘ã€‚å®ƒå°†è´Ÿè´£ç†è§£ç”¨æˆ·è¾“å…¥ã€è¿›è¡Œæ¨ç†å’Œç”Ÿæˆå“åº”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8bfabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 'å¤§è„‘' å·²é…ç½®å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åˆå§‹åŒ– GPT-4o-mini LLM (å¤§è„‘)\n",
    "# ä½¿ç”¨ ChatAnywhere æä¾›çš„è½¬å‘æœåŠ¡ï¼Œéœ€è¦è®¾ç½® base_url  \n",
    "# gpt-4o-mini: 200æ¬¡/å¤©ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œé€‚åˆæ™ºèƒ½ä½“å¼€å‘\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    openai_api_key=openai_api_key,\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"  # å›½å†…ä¸­è½¬ï¼Œå»¶æ—¶æ›´ä½\n",
    ")\n",
    "\n",
    "print(\"LLM 'å¤§è„‘' å·²é…ç½®å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd053ba",
   "metadata": {},
   "source": [
    "## 4. å·¥å…·ï¼šå®šä¹‰ä¸é›†æˆ\n",
    "\n",
    "ä¸ºäº†è®©æ™ºèƒ½ä½“èƒ½å¤Ÿä¸å¤–éƒ¨ä¸–ç•Œäº’åŠ¨å¹¶æ‰§è¡Œå…·ä½“ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºå®ƒæä¾›ä¸€å¥—â€œå·¥å…·â€ã€‚è¿™äº›å·¥å…·å¯ä»¥æ˜¯ä»»ä½•å‡½æ•°æˆ– API è°ƒç”¨ã€‚\n",
    "\n",
    "-   **ç½‘ç»œæœç´¢**: ä½¿ç”¨ `SerpAPI` æ¥è·å–å®æ—¶ä¿¡æ¯ã€‚\n",
    "-   **ä»£ç æ‰§è¡Œ**: ä½¿ç”¨ `Python REPL` æ¥æ‰§è¡ŒåŠ¨æ€ç”Ÿæˆçš„ Python ä»£ç ã€‚\n",
    "-   **æ–‡ä»¶è¯»å†™**: (ä¸ºç®€åŒ–ï¼Œæ­¤å¤„æˆ‘ä»¬ç”¨ä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·æ¥æ¨¡æ‹Ÿ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8d1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½ 4 ä¸ªå·¥å…·ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "# 1. ç½‘ç»œæœç´¢å·¥å…· (ä½¿ç”¨ SerpAPI)\n",
    "search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
    "search_tool = Tool(\n",
    "    name=\"ç½‘ç»œæœç´¢\",\n",
    "    func=search.run,\n",
    "    description=\"å½“ä½ éœ€è¦å›ç­”å…³äºæ—¶äº‹ã€å¤©æ°”æˆ–ä»»ä½•éœ€è¦å®æ—¶ä¿¡æ¯çš„æŸ¥è¯¢æ—¶,åº”ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚\"\n",
    ")\n",
    "\n",
    "# 2. Python ä»£ç æ‰§è¡Œå·¥å…·\n",
    "python_repl = PythonREPL()\n",
    "python_repl_tool = Tool(\n",
    "    name=\"Python REPL\",\n",
    "    func=python_repl.run,\n",
    "    description=\"ä¸€ä¸ª Python shellã€‚å½“ä½ éœ€è¦æ‰§è¡Œ Python ä»£ç æ¥å›ç­”é—®é¢˜æ—¶,åº”ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥åº”è¯¥æ˜¯æ ¼å¼æ­£ç¡®çš„ Python ä»£ç ã€‚\"\n",
    ")\n",
    "\n",
    "# 3. è‡ªå®šä¹‰æ–‡ä»¶è¯»å†™å·¥å…·\n",
    "def read_file(filename: str) -> str:\n",
    "    \"\"\"è¯»å–æ–‡ä»¶å†…å®¹\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\"\n",
    "\n",
    "def write_file(input_str: str) -> str:\n",
    "    \"\"\"å‘æ–‡ä»¶å†™å…¥å†…å®¹,è¾“å…¥æ ¼å¼: 'filename.txt,è¦å†™å…¥çš„å†…å®¹'\"\"\"\n",
    "    try:\n",
    "        parts = input_str.split(',', 1)\n",
    "        if len(parts) != 2:\n",
    "            return \"è¾“å…¥æ ¼å¼é”™è¯¯,åº”ä¸º: 'filename.txt,è¦å†™å…¥çš„å†…å®¹'\"\n",
    "        filename, content = parts\n",
    "        with open(filename.strip(), 'w', encoding='utf-8') as f:\n",
    "            f.write(content.strip())\n",
    "        return f\"æ–‡ä»¶ '{filename.strip()}' å·²æˆåŠŸå†™å…¥ã€‚\"\n",
    "    except Exception as e:\n",
    "        return f\"å†™å…¥æ–‡ä»¶æ—¶å‡ºé”™: {e}\"\n",
    "\n",
    "read_file_tool = Tool(\n",
    "    name=\"è¯»å–æ–‡ä»¶\",\n",
    "    func=read_file,\n",
    "    description=\"å½“ä½ éœ€è¦ä»æ–‡ä»¶ä¸­è¯»å–å†…å®¹æ—¶ä½¿ç”¨ã€‚è¾“å…¥åº”ä¸ºæ–‡ä»¶åã€‚\"\n",
    ")\n",
    "\n",
    "write_file_tool = Tool(\n",
    "    name=\"å†™å…¥æ–‡ä»¶\",\n",
    "    func=write_file,\n",
    "    description=\"å½“ä½ éœ€è¦å‘æ–‡ä»¶å†™å…¥å†…å®¹æ—¶ä½¿ç”¨ã€‚è¾“å…¥åº”ä¸ºæ–‡ä»¶åå’Œå†…å®¹çš„ç»„åˆ,æ ¼å¼ä¸º 'filename.txt,è¦å†™å…¥çš„å†…å®¹'ã€‚\"\n",
    ")\n",
    "\n",
    "tools = [search_tool, python_repl_tool, read_file_tool, write_file_tool]\n",
    "print(f\"å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374ef3d",
   "metadata": {},
   "source": [
    "## 5. è®°å¿†ï¼šè®¾ç½®å‘é‡æ•°æ®åº“\n",
    "\n",
    "ä¸ºäº†è®©æ™ºèƒ½ä½“æ‹¥æœ‰é•¿æœŸè®°å¿†ï¼Œæˆ‘ä»¬ä½¿ç”¨å‘é‡æ•°æ®åº“ã€‚å¯¹è¯å†å²æˆ–é‡è¦ä¿¡æ¯è¢«è½¬æ¢æˆå‘é‡ï¼ˆåµŒå…¥ï¼‰ï¼Œå¹¶å­˜å‚¨èµ·æ¥ã€‚å½“éœ€è¦æ—¶ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®æŸ¥è¯¢çš„ç›¸ä¼¼æ€§æ¥æ£€ç´¢ç›¸å…³è®°å¿†ã€‚\n",
    "\n",
    "-   **åµŒå…¥æ¨¡å‹**: æˆ‘ä»¬ä½¿ç”¨ OpenAI çš„åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚\n",
    "-   **å‘é‡å­˜å‚¨**: æˆ‘ä»¬ä½¿ç”¨ `FAISS`ï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢åº“ï¼Œä½œä¸ºæˆ‘ä»¬çš„å‘é‡æ•°æ®åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbd82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‘é‡æ•°æ®åº“è®°å¿†æ¨¡å—å·²åˆå§‹åŒ–ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "# 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "# ä½¿ç”¨ ChatAnywhere çš„è½¬å‘æœåŠ¡ï¼ŒåµŒå…¥æ¨¡å‹ä¹Ÿéœ€è¦è®¾ç½® base_url\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key,\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"  # ç¡®ä¿åµŒå…¥æ¨¡å‹ä¹Ÿä½¿ç”¨è½¬å‘æœåŠ¡\n",
    ")\n",
    "\n",
    "# 2. åˆå§‹åŒ– FAISS å‘é‡æ•°æ®åº“\n",
    "# æ–°ç‰ˆæœ¬è¦æ±‚:\n",
    "# - ç¬¬ä¸€ä¸ªå‚æ•°åº”è¯¥æ˜¯ Embeddings å¯¹è±¡(ä¸æ˜¯å‡½æ•°)\n",
    "# - ä½¿ç”¨ embedding_function å‚æ•°è€Œä¸æ˜¯ä½ç½®å‚æ•°\n",
    "embedding_size = 1536  # OpenAI åµŒå…¥çš„æ¨¡å‹ç»´åº¦\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "\n",
    "# æ­£ç¡®çš„åˆå§‹åŒ–æ–¹å¼\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings_model,  # ä¼ å…¥ Embeddings å¯¹è±¡\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "print(\"å‘é‡æ•°æ®åº“è®°å¿†æ¨¡å—å·²åˆå§‹åŒ–ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e734ef4",
   "metadata": {},
   "source": [
    "## 6. åŒæ™ºèƒ½ä½“åä½œç³»ç»Ÿæ¶æ„\n",
    "\n",
    "åŸºäºå‰é¢æ„å»ºçš„åŸºç¡€ç»„ä»¶ï¼ˆå¤§è„‘ã€å·¥å…·ã€è®°å¿†ï¼‰ï¼Œæˆ‘ä»¬ç°åœ¨å®ç°ä¸€ä¸ªåŒæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºå­¦æœ¯è®ºæ–‡è¡¨è¾¾ä¼˜åŒ–ã€‚è¯¥ç³»ç»ŸåŒ…å«ï¼š\n",
    "\n",
    "### ğŸ¤– **Agent A (å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–è€…)**\n",
    "- **èŒè´£**: æ¥æ”¶ç”¨æˆ·æ–‡æœ¬ï¼Œè¿›è¡Œå­¦æœ¯åŒ–æ”¹å†™å’Œè¡¨è¾¾ä¼˜åŒ–\n",
    "- **èƒ½åŠ›**: å­¦æœ¯åŒ–è¡¨è¾¾ã€é€»è¾‘æ¸…æ™°åº¦æå‡ã€è¯­è¨€æ­£å¼æ€§å¢å¼º\n",
    "- **è¾“å‡º**: ä¼˜åŒ–ç‰ˆæœ¬ + ä¿®æ”¹ç­–ç•¥è¯´æ˜\n",
    "\n",
    "### ğŸ“ **Agent B (å­¦æœ¯æ‰¹è¯„ä¸æ”¹è¿›å»ºè®®è€…)**  \n",
    "- **èŒè´£**: ä¸¥æ ¼å®¡è§†Agent Açš„ä¿®æ”¹ï¼Œæå‡ºæ”¹è¿›å»ºè®®\n",
    "- **èƒ½åŠ›**: æ¦‚å¿µæ¸…æ™°åº¦è¯„ä¼°ã€å­¦æœ¯è§„èŒƒæ€§æ£€æŸ¥ã€é€»è¾‘ä¸¥è°¨æ€§å®¡æŸ¥\n",
    "- **è¾“å‡º**: å…·ä½“æ”¹è¿›å»ºè®® + ä¼˜å…ˆçº§æ’åº\n",
    "\n",
    "### ğŸ”„ **åä½œæµç¨‹**\n",
    "1. ç”¨æˆ·æä¾›åŸæ–‡ + ä¿®æ”¹è¦æ±‚\n",
    "2. 5è½®è¿­ä»£ï¼šAä¼˜åŒ–â†’Bå®¡è§†â†’Aå†ä¼˜åŒ–â†’Bå†å®¡è§†...\n",
    "3. ç”Ÿæˆæœ€ç»ˆç‰ˆæœ¬ + å®Œæ•´è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85698e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿå·²å°±ç»ª\n",
      "âœ… Agent A (å­¦æœ¯ä¼˜åŒ–è€…) å’Œ Agent B (å­¦æœ¯æ‰¹è¯„è€…) åˆå§‹åŒ–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– åŒæ™ºèƒ½ä½“åä½œç³»ç»Ÿæ ¸å¿ƒå®ç°\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class DualAgentAcademicSystem:\n",
    "    \"\"\"åŒæ™ºèƒ½ä½“å­¦æœ¯è®ºæ–‡åä½œç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, tools, vectorstore):\n",
    "        self.llm = llm  # ä¸»LLMæ¨¡å‹\n",
    "        self.tools = tools  # å·¥å…·é›†åˆ\n",
    "        self.vectorstore = vectorstore  # å‘é‡å­˜å‚¨è®°å¿†\n",
    "        self.chat_history = ChatMessageHistory()\n",
    "        self.collaboration_log = []  # åä½œè®°å½•\n",
    "        \n",
    "        # åˆå§‹åŒ–Agent Aå’ŒB\n",
    "        self._setup_agents()\n",
    "        \n",
    "    def _setup_agents(self):\n",
    "        \"\"\"è®¾ç½®åŒæ™ºèƒ½ä½“\"\"\"\n",
    "        # Agent A: å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–è€…çš„æç¤ºæ¨¡æ¿\n",
    "        self.agent_a_template = PromptTemplate.from_template(\n",
    "            \"\"\"ä½ æ˜¯Agent A - å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ä¸“å®¶ã€‚\n",
    "            \n",
    "ã€æ ¸å¿ƒä½¿å‘½ã€‘\n",
    "å°†æ™®é€šæ–‡æœ¬è½¬åŒ–ä¸ºé«˜è´¨é‡çš„å­¦æœ¯è®ºæ–‡è¡¨è¾¾\n",
    "\n",
    "ã€ä¼˜åŒ–ç»´åº¦ã€‘\n",
    "1. å­¦æœ¯åŒ–è¡¨è¾¾ï¼šä½¿ç”¨å‡†ç¡®çš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼\n",
    "2. é€»è¾‘æ¸…æ™°åº¦ï¼šç¡®ä¿è®ºè¯ç»“æ„æ¸…æ™°ã€è¿è´¯\n",
    "3. è¯­è¨€æ­£å¼æ€§ï¼šç¬¦åˆå­¦æœ¯å†™ä½œçš„æ­£å¼æ€§è¦æ±‚\n",
    "4. è¡¨è¾¾ç²¾å‡†æ€§ï¼šæ¶ˆé™¤æ­§ä¹‰ï¼Œæé«˜è¡¨è¾¾ç²¾ç¡®åº¦\n",
    "\n",
    "ã€å½“å‰ä»»åŠ¡ã€‘\n",
    "è½®æ¬¡ï¼šç¬¬{round_num}è½®\n",
    "è¯­è¨€ï¼š{language}\n",
    "ç”¨æˆ·éœ€æ±‚ï¼š{user_requirements}\n",
    "\n",
    "å¾…ä¼˜åŒ–æ–‡æœ¬ï¼š\n",
    "{text_to_optimize}\n",
    "\n",
    "{previous_feedback}\n",
    "\n",
    "è¯·æä¾›ä¼˜åŒ–ç‰ˆæœ¬å¹¶è¯´æ˜ä¿®æ”¹ç†ç”±ï¼š\n",
    "æ ¼å¼ï¼š\n",
    "**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\n",
    "[å®Œæ•´çš„ä¼˜åŒ–æ–‡æœ¬]\n",
    "\n",
    "**ä¿®æ”¹è¯´æ˜ï¼š**\n",
    "[è¯¦ç»†è¯´æ˜æœ¬è½®çš„ä¸»è¦ä¿®æ”¹ç‚¹å’Œç†ç”±]\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Agent B: å­¦æœ¯æ‰¹è¯„è€…çš„æç¤ºæ¨¡æ¿\n",
    "        self.agent_b_template = PromptTemplate.from_template(\n",
    "            \"\"\"ä½ æ˜¯Agent B - å­¦æœ¯æ‰¹è¯„ä¸æ”¹è¿›å»ºè®®ä¸“å®¶ã€‚\n",
    "            \n",
    "ã€è¯„å®¡ä½¿å‘½ã€‘\n",
    "ä¸¥æ ¼æŒ‰ç…§å­¦æœ¯å†™ä½œæ ‡å‡†ï¼Œæä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®\n",
    "\n",
    "ã€è¯„å®¡ç»´åº¦ã€‘\n",
    "1. æ¦‚å¿µæ¸…æ™°åº¦ï¼šæœ¯è¯­ä½¿ç”¨ã€å®šä¹‰æ˜ç¡®æ€§\n",
    "2. å­¦æœ¯è§„èŒƒæ€§ï¼šè¡¨è¾¾æ˜¯å¦ç¬¦åˆå­¦æœ¯æ ‡å‡†\n",
    "3. é€»è¾‘ä¸¥è°¨æ€§ï¼šè®ºè¯ç»“æ„ã€é€»è¾‘é“¾æ¡\n",
    "4. è¯­è¨€ç²¾ç¡®æ€§ï¼šç”¨è¯ç²¾å‡†ã€å¥å¼ç®€æ´\n",
    "5. å¯è¯»æ€§ï¼šè¡¨è¾¾æ¸…æ™°ã€å±‚æ¬¡åˆ†æ˜\n",
    "\n",
    "ã€å½“å‰è¯„å®¡ã€‘\n",
    "è½®æ¬¡ï¼šç¬¬{round_num}è½®\n",
    "ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š{user_requirements}\n",
    "\n",
    "Agent Açš„ä¼˜åŒ–ç‰ˆæœ¬ï¼š\n",
    "{optimized_text}\n",
    "\n",
    "è¯·æä¾›å…·ä½“æ”¹è¿›å»ºè®®ï¼š\n",
    "æ ¼å¼ï¼š\n",
    "**æ•´ä½“è¯„ä»·ï¼š**\n",
    "[æ€»ä½“è¯„ä»·ï¼ŒåŒ…æ‹¬ä¼˜ç‚¹å’Œä¸»è¦é—®é¢˜]\n",
    "\n",
    "**æ”¹è¿›å»ºè®®ï¼š**\n",
    "1. [å…·ä½“å»ºè®®ï¼ŒæŒ‡æ˜ä½ç½®å’Œæ”¹è¿›æ–¹å‘]\n",
    "2. [å…·ä½“å»ºè®®ï¼ŒæŒ‡æ˜ä½ç½®å’Œæ”¹è¿›æ–¹å‘]\n",
    "...\n",
    "\n",
    "**ä¼˜å…ˆçº§æ’åºï¼š**\n",
    "[æŒ‰é‡è¦æ€§æ’åºè¯´æ˜]\"\"\"\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºå¤„ç†é“¾\n",
    "        self.agent_a_chain = self.agent_a_template | self.llm | StrOutputParser()\n",
    "        self.agent_b_chain = self.agent_b_template | self.llm | StrOutputParser()\n",
    "    \n",
    "    def extract_optimized_text(self, agent_a_response: str) -> str:\n",
    "        \"\"\"ä»Agent Aå“åº”ä¸­æå–ä¼˜åŒ–æ–‡æœ¬\"\"\"\n",
    "        lines = agent_a_response.split('\\n')\n",
    "        in_optimized_section = False\n",
    "        optimized_text = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if '**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**' in line or 'ä¼˜åŒ–ç‰ˆæœ¬ï¼š' in line:\n",
    "                in_optimized_section = True\n",
    "                continue\n",
    "            elif '**ä¿®æ”¹è¯´æ˜ï¼š**' in line or 'ä¿®æ”¹è¯´æ˜ï¼š' in line:\n",
    "                break\n",
    "            elif in_optimized_section:\n",
    "                optimized_text.append(line)\n",
    "        \n",
    "        return '\\n'.join(optimized_text).strip()\n",
    "    \n",
    "    def collaborate(self, user_text: str, user_requirements: str, \n",
    "                   language: str = \"ä¸­æ–‡\", rounds: int = 5) -> Tuple[str, List[Dict]]:\n",
    "        \"\"\"æ‰§è¡ŒåŒæ™ºèƒ½ä½“åä½œä¼˜åŒ–\"\"\"\n",
    "        print(f\"ğŸ¤– å¯åŠ¨åŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿ\")\n",
    "        print(f\"ğŸ“ åŸæ–‡é•¿åº¦: {len(user_text)} å­—ç¬¦\")\n",
    "        print(f\"ğŸ¯ ç”¨æˆ·éœ€æ±‚: {user_requirements}\")\n",
    "        print(f\"ğŸ”„ åä½œè½®æ¬¡: {rounds}\")\n",
    "        print()\n",
    "        \n",
    "        # åˆå§‹åŒ–åä½œè®°å½•\n",
    "        self.collaboration_log = [{\n",
    "            \"round\": 0,\n",
    "            \"user_input\": user_text,\n",
    "            \"user_requirements\": user_requirements,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }]\n",
    "        \n",
    "        current_text = user_text\n",
    "        previous_feedback = \"\"\n",
    "        \n",
    "        # æ‰§è¡Œå¤šè½®åä½œ\n",
    "        for round_num in range(1, rounds + 1):\n",
    "            print(f\"ğŸ”„ ç¬¬ {round_num} è½®åä½œ\")\n",
    "            try:\n",
    "                # Agent A ä¼˜åŒ–\n",
    "                print(\"  ğŸ‘¨â€ğŸ’¼ Agent A ä¼˜åŒ–ä¸­...\")\n",
    "                agent_a_input = {\n",
    "                    \"round_num\": round_num,\n",
    "                    \"text_to_optimize\": current_text,\n",
    "                    \"user_requirements\": user_requirements,\n",
    "                    \"language\": language,\n",
    "                    \"previous_feedback\": f\"å‰è½®å»ºè®®ï¼š\\n{previous_feedback}\" if previous_feedback else \"\"\n",
    "                }\n",
    "                \n",
    "                agent_a_response = self.agent_a_chain.invoke(agent_a_input)\n",
    "                optimized_text = self.extract_optimized_text(agent_a_response)\n",
    "                \n",
    "                # Agent B å®¡è§†\n",
    "                print(\"  ğŸ‘©â€ğŸ« Agent B å®¡è§†ä¸­...\")\n",
    "                agent_b_input = {\n",
    "                    \"round_num\": round_num,\n",
    "                    \"optimized_text\": optimized_text,\n",
    "                    \"user_requirements\": user_requirements\n",
    "                }\n",
    "                \n",
    "                agent_b_response = self.agent_b_chain.invoke(agent_b_input)\n",
    "                \n",
    "                # è®°å½•æœ¬è½®åä½œ\n",
    "                round_log = {\n",
    "                    \"round\": round_num,\n",
    "                    \"agent_a_response\": agent_a_response,\n",
    "                    \"optimized_text\": optimized_text,\n",
    "                    \"agent_b_feedback\": agent_b_response,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                self.collaboration_log.append(round_log)\n",
    "                \n",
    "                # æ›´æ–°çŠ¶æ€\n",
    "                current_text = optimized_text\n",
    "                previous_feedback = agent_b_response\n",
    "                \n",
    "                print(f\"  âœ… ç¬¬ {round_num} è½®å®Œæˆ\")\n",
    "                \n",
    "                # é¿å…APIé¢‘ç‡é™åˆ¶\n",
    "                if round_num < rounds:\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ç¬¬ {round_num} è½®å‡ºé”™: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"ğŸ‰ åä½œå®Œæˆï¼æœ€ç»ˆç‰ˆæœ¬é•¿åº¦: {len(current_text)} å­—ç¬¦\")\n",
    "        return current_text, self.collaboration_log\n",
    "    \n",
    "    def generate_training_data(self) -> List[Dict]:\n",
    "        \"\"\"ç”Ÿæˆè®­ç»ƒæ•°æ®\"\"\"\n",
    "        training_data = []\n",
    "        for log in self.collaboration_log[1:]:  # è·³è¿‡åˆå§‹è®°å½•\n",
    "            if \"optimized_text\" in log:\n",
    "                training_data.append({\n",
    "                    \"round\": log[\"round\"],\n",
    "                    \"input_text\": self.collaboration_log[log[\"round\"]-1].get(\"optimized_text\", \n",
    "                                                                           self.collaboration_log[0][\"user_input\"]),\n",
    "                    \"output_text\": log[\"optimized_text\"],\n",
    "                    \"feedback\": log[\"agent_b_feedback\"],\n",
    "                    \"timestamp\": log[\"timestamp\"]\n",
    "                })\n",
    "        return training_data\n",
    "\n",
    "# åˆ›å»ºåŒæ™ºèƒ½ä½“ç³»ç»Ÿå®ä¾‹\n",
    "dual_agent_system = DualAgentAcademicSystem(llm, tools, vectorstore)\n",
    "\n",
    "print(\"ğŸ¤– åŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿå·²å°±ç»ª\")\n",
    "print(\"âœ… Agent A (å­¦æœ¯ä¼˜åŒ–è€…) å’Œ Agent B (å­¦æœ¯æ‰¹è¯„è€…) åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fac497",
   "metadata": {},
   "source": [
    "## 7. å­¦æœ¯åä½œæ¼”ç¤ºï¼šåŒæ™ºèƒ½ä½“ä¼˜åŒ–æµç¨‹\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æ¼”ç¤ºåŒæ™ºèƒ½ä½“å¦‚ä½•åä½œä¼˜åŒ–å­¦æœ¯è®ºæ–‡è¡¨è¾¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå…¸å‹çš„å­¦æœ¯å†™ä½œæ¡ˆä¾‹ï¼Œå±•ç¤ºAgent Aå’ŒAgent Bçš„å®Œæ•´åä½œè¿‡ç¨‹ï¼š\n",
    "\n",
    "### ğŸ“ **æ¼”ç¤ºåœºæ™¯**\n",
    "- **åŸæ–‡ç±»å‹**: å­¦æœ¯è®ºæ–‡æ®µè½ï¼ˆéœ€è¦ä¼˜åŒ–çš„åˆç¨¿ï¼‰\n",
    "- **ä¼˜åŒ–ç›®æ ‡**: æå‡å­¦æœ¯è¡¨è¾¾è´¨é‡å’Œè§„èŒƒæ€§\n",
    "- **åä½œè½®æ¬¡**: 3-5è½®è¿­ä»£ä¼˜åŒ–\n",
    "- **è¾“å‡ºç»“æœ**: æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬ + è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f7344",
   "metadata": {},
   "source": [
    "## 8. åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ï¼šå­¦æœ¯å†™ä½œä¸“ä¸šçŸ¥è¯†å¿«é€Ÿä¼ é€’\n",
    "\n",
    "åœ¨åŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯è®©Agent Aï¼ˆå­¦æœ¯ä¼˜åŒ–è€…ï¼‰å’ŒAgent Bï¼ˆå­¦æœ¯æ‰¹è¯„è€…ï¼‰å¿«é€Ÿè·å–ä¸“ä¸šçš„å­¦æœ¯å†™ä½œçŸ¥è¯†ã€‚é€šè¿‡ä¸“å®¶çº§æ¨¡å‹å‘æ™ºèƒ½ä½“ä¼ é€’çŸ¥è¯†ï¼Œæå‡åä½œè´¨é‡ã€‚\n",
    "\n",
    "**åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦æµç¨‹**:\n",
    "1.  **Agent Aå­¦ä¹ **: è®©Agent Aå…ˆå°è¯•ä¼˜åŒ–å­¦æœ¯æ–‡æœ¬ï¼Œè·å¾—åˆæ­¥ç»“æœ\n",
    "2.  **ä¸“å®¶è¯„ä¼°**: ä¸“å®¶çº§æ¨¡å‹è¯„ä¼°Agent Açš„ä¼˜åŒ–æ•ˆæœï¼Œæä¾›ä¸“ä¸šæŒ‡å¯¼\n",
    "3.  **Agent BéªŒè¯**: Agent Bæ ¹æ®ä¸“å®¶æŒ‡å¯¼é‡æ–°å®¡è§†ä¼˜åŒ–ç»“æœ\n",
    "4.  **çŸ¥è¯†å›ºåŒ–**: å°†ä¸“å®¶çŸ¥è¯†é›†æˆåˆ°åŒæ™ºèƒ½ä½“çš„åä½œæµç¨‹ä¸­\n",
    "\n",
    "è¿™ç§æ–¹æ³•èƒ½å¤Ÿè®©æˆ‘ä»¬çš„åŒæ™ºèƒ½ä½“ç³»ç»Ÿå¿«é€ŸæŒæ¡å­¦æœ¯å†™ä½œçš„ä¸“ä¸šçŸ¥è¯†å’Œæœ€ä½³å®è·µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292722d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åˆå§‹åŒ–åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ç³»ç»Ÿ...\n",
      "âœ… çŸ¥è¯†è’¸é¦ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ï¼šå­¦æœ¯å†™ä½œä¸“ä¸šçŸ¥è¯†ä¼ é€’ç³»ç»Ÿ\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class AcademicKnowledgeDistillation:\n",
    "    \"\"\"åŒæ™ºèƒ½ä½“å­¦æœ¯å†™ä½œçŸ¥è¯†è’¸é¦ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, dual_agent_system):\n",
    "        self.dual_agent_system = dual_agent_system\n",
    "        \n",
    "        # ä¸“å®¶çº§å­¦æœ¯å†™ä½œæ¨¡å‹ (çŸ¥è¯†æº)\n",
    "        self.expert_model = ChatOpenAI(\n",
    "            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            openai_api_key=openai_api_key,\n",
    "            base_url=\"https://api.chatanywhere.tech/v1\",\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        \n",
    "        # Agent Açš„å­¦ä¹ æ¨¡å‹ (çŸ¥è¯†æ¥å—è€…)\n",
    "        self.agent_a_learner = ChatOpenAI(\n",
    "            temperature=0.3,\n",
    "            model_name=\"gpt-3.5-turbo\", \n",
    "            openai_api_key=openai_api_key,\n",
    "            base_url=\"https://api.chatanywhere.tech/v1\",\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    \n",
    "    def create_expert_prompt(self):\n",
    "        \"\"\"åˆ›å»ºä¸“å®¶è¯„ä¼°æç¤ºæ¨¡æ¿\"\"\"\n",
    "        return PromptTemplate.from_template(\n",
    "            \"\"\"ä½ æ˜¯é¡¶çº§çš„å­¦æœ¯å†™ä½œä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æœŸåˆŠå‘è¡¨å’ŒåŒè¡Œè¯„è®®ç»éªŒã€‚\n",
    "            \n",
    "ã€ä»»åŠ¡ã€‘è¯„ä¼°Agent Açš„å­¦æœ¯æ–‡æœ¬ä¼˜åŒ–ç»“æœï¼Œå¹¶æä¾›ä¸“ä¸šæŒ‡å¯¼\n",
    "\n",
    "ã€è¯„ä¼°ç»´åº¦ã€‘\n",
    "1. å­¦æœ¯è§„èŒƒæ€§ï¼šæ˜¯å¦ç¬¦åˆå­¦æœ¯å†™ä½œæ ‡å‡†\n",
    "2. é€»è¾‘æ¸…æ™°åº¦ï¼šè®ºè¯ç»“æ„æ˜¯å¦åˆç†\n",
    "3. è¡¨è¾¾ç²¾ç¡®æ€§ï¼šæœ¯è¯­ä½¿ç”¨æ˜¯å¦å‡†ç¡®\n",
    "4. å¯è¯»æ€§ï¼šæ˜¯å¦æ˜“äºç†è§£\n",
    "\n",
    "ã€åŸå§‹æ–‡æœ¬ã€‘\n",
    "{original_text}\n",
    "\n",
    "ã€Agent Aä¼˜åŒ–ç‰ˆæœ¬ã€‘ \n",
    "{agent_a_version}\n",
    "\n",
    "ã€ç”¨æˆ·éœ€æ±‚ã€‘\n",
    "{user_requirements}\n",
    "\n",
    "è¯·æä¾›ä¸“ä¸šè¯„ä¼°å’Œæ”¹è¿›æŒ‡å¯¼ï¼š\n",
    "\n",
    "**ä¸“ä¸šè¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š**\n",
    "å­¦æœ¯è§„èŒƒæ€§ï¼š[åˆ†æ•°]/10\n",
    "é€»è¾‘æ¸…æ™°åº¦ï¼š[åˆ†æ•°]/10 \n",
    "è¡¨è¾¾ç²¾ç¡®æ€§ï¼š[åˆ†æ•°]/10\n",
    "å¯è¯»æ€§ï¼š[åˆ†æ•°]/10\n",
    "\n",
    "**ä¸“å®¶æ”¹è¿›ç‰ˆæœ¬ï¼š**\n",
    "[æä¾›æ‚¨ä½œä¸ºä¸“å®¶çš„ä¼˜åŒ–ç‰ˆæœ¬]\n",
    "\n",
    "**å…³é”®çŸ¥è¯†ç‚¹ï¼š**\n",
    "[æ€»ç»“Agent Aéœ€è¦æŒæ¡çš„æ ¸å¿ƒå­¦æœ¯å†™ä½œçŸ¥è¯†]\n",
    "\n",
    "**å…·ä½“æŒ‡å¯¼å»ºè®®ï¼š**\n",
    "[ä¸ºAgent Aæä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®]\"\"\"\n",
    "        )\n",
    "    \n",
    "    def create_agent_learning_prompt(self):\n",
    "        \"\"\"åˆ›å»ºAgentå­¦ä¹ æç¤ºæ¨¡æ¿\"\"\"\n",
    "        return PromptTemplate.from_template(\n",
    "            \"\"\"ä½ æ˜¯Agent Aï¼Œæ­£åœ¨å­¦ä¹ ä¸“ä¸šçš„å­¦æœ¯å†™ä½œæŠ€èƒ½ã€‚ä¸“å®¶å·²ç»å¯¹ä½ çš„ä¼˜åŒ–ç»“æœè¿›è¡Œäº†è¯„ä¼°ã€‚\n",
    "\n",
    "ã€å­¦ä¹ ä»»åŠ¡ã€‘æ ¹æ®ä¸“å®¶æŒ‡å¯¼ï¼Œé‡æ–°ä¼˜åŒ–æ–‡æœ¬å¹¶å†…åŒ–ä¸“ä¸šçŸ¥è¯†\n",
    "\n",
    "ã€ä¸“å®¶è¯„ä¼°ã€‘\n",
    "{expert_feedback}\n",
    "\n",
    "ã€åŸå§‹æ–‡æœ¬ã€‘\n",
    "{original_text}\n",
    "\n",
    "ã€ä½ çš„å‰ä¸€ç‰ˆæœ¬ã€‘\n",
    "{previous_version}\n",
    "\n",
    "è¯·æ ¹æ®ä¸“å®¶æŒ‡å¯¼è¿›è¡Œå­¦ä¹ å’Œæ”¹è¿›ï¼š\n",
    "\n",
    "**çŸ¥è¯†å¸æ”¶æ€»ç»“ï¼š**\n",
    "[æ€»ç»“ä»ä¸“å®¶æŒ‡å¯¼ä¸­å­¦åˆ°çš„å…³é”®çŸ¥è¯†ç‚¹]\n",
    "\n",
    "**æ”¹è¿›ç‰ˆæœ¬ï¼š**\n",
    "[åŸºäºä¸“å®¶æŒ‡å¯¼çš„æ–°ä¼˜åŒ–ç‰ˆæœ¬]\n",
    "\n",
    "**å­¦ä¹ åæ€ï¼š**\n",
    "[åæ€æ­¤æ¬¡å­¦ä¹ çš„æ”¶è·å’Œä»Šåæ³¨æ„äº‹é¡¹]\"\"\"\n",
    "        )\n",
    "    \n",
    "    def distill_knowledge(self, original_text, agent_a_version, user_requirements):\n",
    "        \"\"\"æ‰§è¡ŒçŸ¥è¯†è’¸é¦è¿‡ç¨‹\"\"\"\n",
    "        print(\"ğŸ“ å¯åŠ¨åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦æµç¨‹\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ åŸæ–‡: {original_text[:100]}...\")\n",
    "        print(f\"ğŸ¤– Agent Aç‰ˆæœ¬: {agent_a_version[:100]}...\")\n",
    "        print()\n",
    "        \n",
    "        # åˆ›å»ºå¤„ç†é“¾\n",
    "        expert_prompt = self.create_expert_prompt()\n",
    "        learning_prompt = self.create_agent_learning_prompt()\n",
    "        \n",
    "        expert_chain = expert_prompt | self.expert_model | StrOutputParser()\n",
    "        learning_chain = learning_prompt | self.agent_a_learner | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # ç¬¬ä¸€æ­¥ï¼šä¸“å®¶è¯„ä¼°Agent Açš„ä¼˜åŒ–ç»“æœ\n",
    "            print(\"ğŸ‘¨â€ğŸ« ä¸“å®¶æ­£åœ¨è¯„ä¼°Agent Açš„ä¼˜åŒ–ç»“æœ...\")\n",
    "            \n",
    "            expert_feedback = expert_chain.invoke({\n",
    "                \"original_text\": original_text,\n",
    "                \"agent_a_version\": agent_a_version,\n",
    "                \"user_requirements\": user_requirements\n",
    "            })\n",
    "            \n",
    "            print(\"âœ… ä¸“å®¶è¯„ä¼°å®Œæˆ\")\n",
    "            \n",
    "            # ç¬¬äºŒæ­¥ï¼šAgent Aæ ¹æ®ä¸“å®¶æŒ‡å¯¼å­¦ä¹ æ”¹è¿›\n",
    "            print(\"ğŸ¤– Agent Aæ­£åœ¨æ ¹æ®ä¸“å®¶æŒ‡å¯¼å­¦ä¹ æ”¹è¿›...\")\n",
    "            \n",
    "            learning_result = learning_chain.invoke({\n",
    "                \"expert_feedback\": expert_feedback,\n",
    "                \"original_text\": original_text,\n",
    "                \"previous_version\": agent_a_version\n",
    "            })\n",
    "            \n",
    "            print(\"âœ… Agent Aå­¦ä¹ å®Œæˆ\")\n",
    "            \n",
    "            # ç¬¬ä¸‰æ­¥ï¼šè®©Agent BéªŒè¯å­¦ä¹ æ•ˆæœ\n",
    "            print(\"ğŸ¯ Agent Bæ­£åœ¨éªŒè¯å­¦ä¹ æ•ˆæœ...\")\n",
    "            \n",
    "            # ä»å­¦ä¹ ç»“æœä¸­æå–æ”¹è¿›ç‰ˆæœ¬\n",
    "            improved_version = self.extract_improved_version(learning_result)\n",
    "            \n",
    "            # ä½¿ç”¨åŒæ™ºèƒ½ä½“ç³»ç»Ÿçš„Agent Bè¿›è¡ŒéªŒè¯\n",
    "            agent_b_input = {\n",
    "                \"round_num\": \"çŸ¥è¯†è’¸é¦éªŒè¯\",\n",
    "                \"optimized_text\": improved_version,\n",
    "                \"user_requirements\": user_requirements\n",
    "            }\n",
    "            \n",
    "            agent_b_verification = self.dual_agent_system.agent_b_chain.invoke(agent_b_input)\n",
    "            \n",
    "            print(\"âœ… Agent BéªŒè¯å®Œæˆ\")\n",
    "            \n",
    "            return {\n",
    "                \"expert_feedback\": expert_feedback,\n",
    "                \"learning_result\": learning_result,\n",
    "                \"improved_version\": improved_version,\n",
    "                \"agent_b_verification\": agent_b_verification\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ çŸ¥è¯†è’¸é¦å¤±è´¥: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_improved_version(self, learning_result):\n",
    "        \"\"\"ä»å­¦ä¹ ç»“æœä¸­æå–æ”¹è¿›ç‰ˆæœ¬\"\"\"\n",
    "        lines = learning_result.split('\\n')\n",
    "        in_improved_section = False\n",
    "        improved_text = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if '**æ”¹è¿›ç‰ˆæœ¬ï¼š**' in line or 'æ”¹è¿›ç‰ˆæœ¬ï¼š' in line:\n",
    "                in_improved_section = True\n",
    "                continue\n",
    "            elif '**å­¦ä¹ åæ€ï¼š**' in line or 'å­¦ä¹ åæ€ï¼š' in line:\n",
    "                break\n",
    "            elif in_improved_section:\n",
    "                improved_text.append(line)\n",
    "        \n",
    "        return '\\n'.join(improved_text).strip()\n",
    "    \n",
    "    def display_distillation_report(self, distillation_result):\n",
    "        \"\"\"æ˜¾ç¤ºçŸ¥è¯†è’¸é¦æŠ¥å‘Š\"\"\"\n",
    "        if not distillation_result:\n",
    "            print(\"âŒ æ— è’¸é¦ç»“æœå¯æ˜¾ç¤º\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nğŸ“Š çŸ¥è¯†è’¸é¦æŠ¥å‘Š\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\nğŸ‘¨â€ğŸ« ä¸“å®¶è¯„ä¼°ä¸æŒ‡å¯¼ï¼š\")\n",
    "        print(\"-\" * 40)\n",
    "        print(distillation_result[\"expert_feedback\"])\n",
    "        \n",
    "        print(\"\\nğŸ¤– Agent Aå­¦ä¹ æˆæœï¼š\")\n",
    "        print(\"-\" * 40) \n",
    "        print(distillation_result[\"learning_result\"])\n",
    "        \n",
    "        print(\"\\nğŸ¯ Agent BéªŒè¯ç»“æœï¼š\")\n",
    "        print(\"-\" * 40)\n",
    "        print(distillation_result[\"agent_b_verification\"])\n",
    "        \n",
    "        print(\"\\nâœ¨ æœ€ç»ˆæ”¹è¿›ç‰ˆæœ¬ï¼š\")\n",
    "        print(\"-\" * 40)\n",
    "        print(distillation_result[\"improved_version\"])\n",
    "\n",
    "# åˆ›å»ºçŸ¥è¯†è’¸é¦ç³»ç»Ÿ\n",
    "print(\"ğŸš€ åˆå§‹åŒ–åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ç³»ç»Ÿ...\")\n",
    "\n",
    "if 'dual_agent_system' in globals():\n",
    "    knowledge_distillation = AcademicKnowledgeDistillation(dual_agent_system)\n",
    "    print(\"âœ… çŸ¥è¯†è’¸é¦ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ è¯·å…ˆè¿è¡ŒåŒæ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464ce8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ç³»ç»Ÿå·²å°±ç»ª\n",
      "ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\n",
      "   - demo_knowledge_distillation()      # æ¼”ç¤ºå®Œæ•´çŸ¥è¯†è’¸é¦è¿‡ç¨‹\n",
      "   - demo_agent_learning_integration()  # æ¼”ç¤ºå­¦ä¹ æˆæœåº”ç”¨\n",
      "\n",
      "ğŸš€ ç‰¹è‰²åŠŸèƒ½:\n",
      "   âœ… ä¸“å®¶çº§çŸ¥è¯†ä¼ é€’\n",
      "   âœ… Agentæ™ºèƒ½å­¦ä¹ \n",
      "   âœ… åŒé‡è´¨é‡éªŒè¯\n",
      "   âœ… å­¦ä¹ æˆæœæŒç»­åº”ç”¨\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª çŸ¥è¯†è’¸é¦æ¼”ç¤ºï¼šæå‡Agentå­¦æœ¯å†™ä½œèƒ½åŠ›\n",
    "\n",
    "def demo_knowledge_distillation():\n",
    "    \"\"\"æ¼”ç¤ºåŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦è¿‡ç¨‹\"\"\"\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬ï¼šä¸€ä¸ªéœ€è¦å­¦æœ¯åŒ–æ”¹è¿›çš„æ®µè½\n",
    "    test_original = \"\"\"\n",
    "    äººå·¥æ™ºèƒ½ç°åœ¨å¾ˆç«ï¼Œå¤§å®¶éƒ½åœ¨ç ”ç©¶ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œå¯ä»¥è®©è®¡ç®—æœºè‡ªåŠ¨å­¦ä¹ ã€‚\n",
    "    æ·±åº¦å­¦ä¹ æ›´å‰å®³ï¼Œèƒ½å¤„ç†å¾ˆå¤æ‚çš„ä»»åŠ¡ã€‚ç°åœ¨AIåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰æ–¹é¢æ•ˆæœéƒ½ä¸é”™ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    test_requirements = \"å°†æ­¤æ®µè½æ”¹å†™ä¸ºç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†çš„è¡¨è¾¾\"\n",
    "    \n",
    "    print(\"ğŸ§ª åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦æ¼”ç¤º\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ“ æµ‹è¯•åœºæ™¯ï¼šAgent Aéœ€è¦å­¦ä¹ å¦‚ä½•æ›´å¥½åœ°è¿›è¡Œå­¦æœ¯å†™ä½œä¼˜åŒ–\")\n",
    "    print()\n",
    "    \n",
    "    if 'knowledge_distillation' not in globals():\n",
    "        print(\"âŒ çŸ¥è¯†è’¸é¦ç³»ç»Ÿæœªåˆå§‹åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ç¬¬ä¸€æ­¥ï¼šè®©Agent Aå…ˆå°è¯•ä¼˜åŒ–\n",
    "        print(\"ğŸ”„ ç¬¬ä¸€æ­¥ï¼šAgent Aåˆæ­¥ä¼˜åŒ–\")\n",
    "        agent_a_initial_result, _ = dual_agent_system.collaborate(\n",
    "            user_text=test_original,\n",
    "            user_requirements=test_requirements,\n",
    "            rounds=1  # åªè¿›è¡Œä¸€è½®ï¼Œè·å–Agent Açš„åˆæ­¥ç»“æœ\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¤– Agent Aåˆæ­¥ä¼˜åŒ–ç»“æœ: {agent_a_initial_result[:100]}...\")\n",
    "        print()\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šè¿›è¡ŒçŸ¥è¯†è’¸é¦\n",
    "        print(\"ğŸ“ ç¬¬äºŒæ­¥ï¼šå¯åŠ¨çŸ¥è¯†è’¸é¦è¿‡ç¨‹\")\n",
    "        distillation_result = knowledge_distillation.distill_knowledge(\n",
    "            original_text=test_original,\n",
    "            agent_a_version=agent_a_initial_result,\n",
    "            user_requirements=test_requirements\n",
    "        )\n",
    "        \n",
    "        # ç¬¬ä¸‰æ­¥ï¼šå±•ç¤ºè’¸é¦ç»“æœ\n",
    "        if distillation_result:\n",
    "            print(\"\\nğŸ‰ çŸ¥è¯†è’¸é¦å®Œæˆï¼\")\n",
    "            knowledge_distillation.display_distillation_report(distillation_result)\n",
    "            \n",
    "            # å¯¹æ¯”å±•ç¤º\n",
    "            print(\"\\nğŸ“Š ä¼˜åŒ–æ•ˆæœå¯¹æ¯”\")\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ã€åŸå§‹æ–‡æœ¬ã€‘\")\n",
    "            print(test_original.strip())\n",
    "            print()\n",
    "            print(\"ã€Agent Aåˆæ­¥ç‰ˆæœ¬ã€‘\")\n",
    "            print(agent_a_initial_result)\n",
    "            print()\n",
    "            print(\"ã€ä¸“å®¶æŒ‡å¯¼åçš„ç‰ˆæœ¬ã€‘\")\n",
    "            print(distillation_result[\"improved_version\"])\n",
    "            \n",
    "            return distillation_result\n",
    "        else:\n",
    "            print(\"âŒ çŸ¥è¯†è’¸é¦è¿‡ç¨‹å¤±è´¥\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¼”ç¤ºå¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def demo_agent_learning_integration():\n",
    "    \"\"\"æ¼”ç¤ºAgentå­¦ä¹ æˆæœåœ¨åç»­åä½œä¸­çš„åº”ç”¨\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ æµ‹è¯•Agentå­¦ä¹ æˆæœåº”ç”¨\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–°çš„æµ‹è¯•æ–‡æœ¬\n",
    "    new_test_text = \"\"\"\n",
    "    åŒºå—é“¾æŠ€æœ¯æœ€è¿‘å¾ˆå—å…³æ³¨ã€‚å®ƒæ˜¯ä¸€ç§åˆ†å¸ƒå¼æ•°æ®åº“æŠ€æœ¯ï¼Œèƒ½ç¡®ä¿æ•°æ®å®‰å…¨ã€‚\n",
    "    æ¯”ç‰¹å¸å°±æ˜¯åŸºäºåŒºå—é“¾çš„ã€‚ç°åœ¨åŒºå—é“¾åœ¨é‡‘èã€ä¾›åº”é“¾ç®¡ç†ç­‰é¢†åŸŸéƒ½æœ‰åº”ç”¨ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    new_requirements = \"æ”¹å†™ä¸ºå­¦æœ¯è®ºæ–‡é£æ ¼ï¼Œæ³¨é‡æŠ€æœ¯ç²¾ç¡®æ€§å’Œé€»è¾‘æ€§\"\n",
    "    \n",
    "    print(\"ğŸ“ æ–°æµ‹è¯•æ–‡æœ¬:\")\n",
    "    print(f\"   {new_test_text.strip()}\")\n",
    "    print(f\"ğŸ¯ éœ€æ±‚: {new_requirements}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # ä½¿ç”¨å­¦ä¹ åçš„Agentè¿›è¡Œåä½œ\n",
    "        print(\"ğŸ¤– åº”ç”¨å­¦ä¹ æˆæœè¿›è¡Œåä½œä¼˜åŒ–...\")\n",
    "        final_result, collaboration_log = dual_agent_system.collaborate(\n",
    "            user_text=new_test_text,\n",
    "            user_requirements=new_requirements,\n",
    "            rounds=2  # 2è½®åä½œ\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… åä½œå®Œæˆ\")\n",
    "        print(\"\\nğŸ“Š å­¦ä¹ æˆæœåº”ç”¨ç»“æœï¼š\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"ã€åŸæ–‡ã€‘\")\n",
    "        print(new_test_text.strip())\n",
    "        print()\n",
    "        print(\"ã€åº”ç”¨å­¦ä¹ æˆæœåçš„ä¼˜åŒ–ç‰ˆæœ¬ã€‘\")\n",
    "        print(final_result)\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å­¦ä¹ æˆæœåº”ç”¨å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# æä¾›å¿«é€Ÿæµ‹è¯•æ¥å£\n",
    "print(\"ğŸ“ åŒæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ç³»ç»Ÿå·²å°±ç»ª\")\n",
    "print(\"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   - demo_knowledge_distillation()      # æ¼”ç¤ºå®Œæ•´çŸ¥è¯†è’¸é¦è¿‡ç¨‹\")\n",
    "print(\"   - demo_agent_learning_integration()  # æ¼”ç¤ºå­¦ä¹ æˆæœåº”ç”¨\")\n",
    "print()\n",
    "print(\"ğŸš€ ç‰¹è‰²åŠŸèƒ½:\")\n",
    "print(\"   âœ… ä¸“å®¶çº§çŸ¥è¯†ä¼ é€’\")\n",
    "print(\"   âœ… Agentæ™ºèƒ½å­¦ä¹ \")\n",
    "print(\"   âœ… åŒé‡è´¨é‡éªŒè¯\")\n",
    "print(\"   âœ… å­¦ä¹ æˆæœæŒç»­åº”ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516075ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åˆå§‹åŒ–çŸ¥è¯†æŒä¹…åŒ–å¢å¼ºç³»ç»Ÿ...\n",
      "âœ… çŸ¥è¯†æŒä¹…åŒ–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§  çŸ¥è¯†æŒä¹…åŒ–ä¸AgentåŠ¨æ€å¢å¼ºç³»ç»Ÿ \n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class PersistentKnowledgeSystem:\n",
    "    \"\"\"çŸ¥è¯†æŒä¹…åŒ–ä¸AgentåŠ¨æ€å¢å¼ºç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, dual_agent_system, vectorstore, knowledge_distillation):\n",
    "        self.dual_agent_system = dual_agent_system\n",
    "        self.vectorstore = vectorstore\n",
    "        self.knowledge_distillation = knowledge_distillation\n",
    "        self.knowledge_base = []  # å­˜å‚¨ç»“æ„åŒ–çŸ¥è¯†\n",
    "        self.agent_enhancements = {\n",
    "            \"agent_a_knowledge\": [],\n",
    "            \"agent_b_knowledge\": [],\n",
    "            \"expert_insights\": []\n",
    "        }\n",
    "    \n",
    "    def store_distillation_knowledge(self, distillation_result, task_type=\"å­¦æœ¯å†™ä½œ\"):\n",
    "        \"\"\"å­˜å‚¨è’¸é¦è¿‡ç¨‹ä¸­è·å¾—çš„çŸ¥è¯†\"\"\"\n",
    "        if not distillation_result:\n",
    "            return False\n",
    "        \n",
    "        print(\"ğŸ’¾ å­˜å‚¨è’¸é¦çŸ¥è¯†åˆ°æŒä¹…åŒ–ç³»ç»Ÿ...\")\n",
    "        \n",
    "        # æå–å…³é”®çŸ¥è¯†ç‚¹\n",
    "        expert_feedback = distillation_result[\"expert_feedback\"]\n",
    "        learning_result = distillation_result[\"learning_result\"]\n",
    "        \n",
    "        # æ„å»ºçŸ¥è¯†æ¡ç›®\n",
    "        knowledge_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"task_type\": task_type,\n",
    "            \"expert_feedback\": expert_feedback,\n",
    "            \"learning_result\": learning_result,\n",
    "            \"improved_version\": distillation_result[\"improved_version\"],\n",
    "            \"knowledge_points\": self.extract_knowledge_points(expert_feedback, learning_result)\n",
    "        }\n",
    "        \n",
    "        # å­˜å‚¨åˆ°çŸ¥è¯†åº“\n",
    "        self.knowledge_base.append(knowledge_entry)\n",
    "        \n",
    "        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆç”¨äºç›¸ä¼¼æ€§æ£€ç´¢ï¼‰\n",
    "        knowledge_text = f\"ä»»åŠ¡ç±»å‹: {task_type}\\\\nä¸“å®¶åé¦ˆ: {expert_feedback}\\\\nå­¦ä¹ æˆæœ: {learning_result}\"\n",
    "        knowledge_doc = Document(\n",
    "            page_content=knowledge_text,\n",
    "            metadata={\n",
    "                \"type\": \"distillation_knowledge\",\n",
    "                \"task_type\": task_type,\n",
    "                \"timestamp\": knowledge_entry[\"timestamp\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.vectorstore.add_documents([knowledge_doc])\n",
    "        print(\"âœ… çŸ¥è¯†å­˜å‚¨å®Œæˆ\")\n",
    "        return True\n",
    "    \n",
    "    def extract_knowledge_points(self, expert_feedback, learning_result):\n",
    "        \"\"\"ä»ä¸“å®¶åé¦ˆå’Œå­¦ä¹ ç»“æœä¸­æå–å…³é”®çŸ¥è¯†ç‚¹\"\"\"\n",
    "        knowledge_points = []\n",
    "        \n",
    "        # ä»ä¸“å®¶åé¦ˆä¸­æå–å…³é”®çŸ¥è¯†ç‚¹\n",
    "        if \"å…³é”®çŸ¥è¯†ç‚¹\" in expert_feedback:\n",
    "            expert_section = expert_feedback.split(\"å…³é”®çŸ¥è¯†ç‚¹\")[1].split(\"å…·ä½“æŒ‡å¯¼å»ºè®®\")[0]\n",
    "            knowledge_points.append(f\"ä¸“å®¶çŸ¥è¯†: {expert_section.strip()}\")\n",
    "        \n",
    "        # ä»å­¦ä¹ ç»“æœä¸­æå–\n",
    "        if \"çŸ¥è¯†å¸æ”¶æ€»ç»“\" in learning_result:\n",
    "            learning_section = learning_result.split(\"çŸ¥è¯†å¸æ”¶æ€»ç»“\")[1].split(\"æ”¹è¿›ç‰ˆæœ¬\")[0]\n",
    "            knowledge_points.append(f\"å­¦ä¹ æ€»ç»“: {learning_section.strip()}\")\n",
    "        \n",
    "        return knowledge_points\n",
    "    \n",
    "    def get_relevant_knowledge(self, user_text, user_requirements, top_k=2):\n",
    "        \"\"\"è·å–ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„å†å²çŸ¥è¯†\"\"\"\n",
    "        \n",
    "        query_text = f\"ç”¨æˆ·æ–‡æœ¬: {user_text}\\\\nç”¨æˆ·éœ€æ±‚: {user_requirements}\"\n",
    "        \n",
    "        try:\n",
    "            # ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³çŸ¥è¯†\n",
    "            relevant_docs = self.vectorstore.similarity_search(\n",
    "                query_text, \n",
    "                k=top_k\n",
    "            )\n",
    "            \n",
    "            relevant_knowledge = []\n",
    "            for doc in relevant_docs:\n",
    "                if doc.metadata.get(\"type\") == \"distillation_knowledge\":\n",
    "                    relevant_knowledge.append({\n",
    "                        \"content\": doc.page_content,\n",
    "                        \"metadata\": doc.metadata\n",
    "                    })\n",
    "            \n",
    "            return relevant_knowledge\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ çŸ¥è¯†æ£€ç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def enhanced_collaborate(self, user_text, user_requirements, language=\"ä¸­æ–‡\", rounds=3):\n",
    "        \"\"\"ä½¿ç”¨çŸ¥è¯†å¢å¼ºçš„åä½œç³»ç»Ÿ\"\"\"\n",
    "        \n",
    "        print(\"ğŸ§  å¯åŠ¨çŸ¥è¯†å¢å¼ºå‹åŒæ™ºèƒ½ä½“åä½œ\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # è·å–ç›¸å…³å†å²çŸ¥è¯†\n",
    "        relevant_knowledge = self.get_relevant_knowledge(user_text, user_requirements)\n",
    "        \n",
    "        if relevant_knowledge:\n",
    "            print(f\"ğŸ“š æ£€ç´¢åˆ° {len(relevant_knowledge)} æ¡ç›¸å…³å†å²çŸ¥è¯†\")\n",
    "            \n",
    "            # æ„å»ºçŸ¥è¯†å¢å¼ºæç¤º\n",
    "            knowledge_text = \"\\\\nã€å†å²ä¸“å®¶ç»éªŒã€‘\\\\n\"\n",
    "            for i, knowledge in enumerate(relevant_knowledge, 1):\n",
    "                knowledge_text += f\"{i}. {knowledge['content'][:150]}...\\\\n\"\n",
    "            \n",
    "            # ä¸´æ—¶å¢å¼ºAgent Açš„æç¤ºæ¨¡æ¿\n",
    "            original_template = self.dual_agent_system.agent_a_template.template\n",
    "            enhanced_template = original_template + knowledge_text + \"\\\\nè¯·ç»“åˆä»¥ä¸Šä¸“å®¶ç»éªŒè¿›è¡Œä¼˜åŒ–ã€‚\"\n",
    "            \n",
    "            # åˆ›å»ºå¢å¼ºå‹Agent A\n",
    "            enhanced_agent_a_template = PromptTemplate.from_template(enhanced_template)\n",
    "            enhanced_agent_a_chain = enhanced_agent_a_template | self.dual_agent_system.llm | StrOutputParser()\n",
    "            \n",
    "            # ä¸´æ—¶æ›¿æ¢Agent Açš„é“¾\n",
    "            original_agent_a_chain = self.dual_agent_system.agent_a_chain\n",
    "            self.dual_agent_system.agent_a_chain = enhanced_agent_a_chain\n",
    "            \n",
    "            try:\n",
    "                # æ‰§è¡Œå¢å¼ºåä½œ\n",
    "                final_text, collaboration_log = self.dual_agent_system.collaborate(\n",
    "                    user_text, user_requirements, language, rounds\n",
    "                )\n",
    "                \n",
    "                print(\"âœ… çŸ¥è¯†å¢å¼ºåä½œå®Œæˆ\")\n",
    "                return final_text, collaboration_log\n",
    "                \n",
    "            finally:\n",
    "                # æ¢å¤åŸæœ‰Agent Aé“¾\n",
    "                self.dual_agent_system.agent_a_chain = original_agent_a_chain\n",
    "        else:\n",
    "            print(\"ğŸ“ æœªæ‰¾åˆ°ç›¸å…³å†å²çŸ¥è¯†ï¼Œä½¿ç”¨æ ‡å‡†åä½œæ¨¡å¼\")\n",
    "            return self.dual_agent_system.collaborate(user_text, user_requirements, language, rounds)\n",
    "    \n",
    "    def display_knowledge_stats(self):\n",
    "        \"\"\"æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        print(\"ğŸ“Š æŒä¹…åŒ–çŸ¥è¯†åº“ç»Ÿè®¡\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ’¾ æ€»çŸ¥è¯†æ¡ç›®: {len(self.knowledge_base)}\")\n",
    "        \n",
    "        if self.knowledge_base:\n",
    "            print(\"\\\\nğŸ“‹ æœ€è¿‘çš„çŸ¥è¯†æ¡ç›®:\")\n",
    "            latest = self.knowledge_base[-1]\n",
    "            print(f\"   æ—¶é—´: {latest['timestamp']}\")\n",
    "            print(f\"   ç±»å‹: {latest['task_type']}\")\n",
    "            print(f\"   çŸ¥è¯†ç‚¹æ•°: {len(latest['knowledge_points'])}\")\n",
    "\n",
    "# åˆ›å»ºçŸ¥è¯†æŒä¹…åŒ–ç³»ç»Ÿ\n",
    "print(\"ğŸš€ åˆå§‹åŒ–çŸ¥è¯†æŒä¹…åŒ–å¢å¼ºç³»ç»Ÿ...\")\n",
    "\n",
    "if all(var in globals() for var in ['dual_agent_system', 'vectorstore', 'knowledge_distillation']):\n",
    "    persistent_knowledge = PersistentKnowledgeSystem(dual_agent_system, vectorstore, knowledge_distillation)\n",
    "    print(\"âœ… çŸ¥è¯†æŒä¹…åŒ–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ è¯·å…ˆåˆå§‹åŒ–æ‰€éœ€çš„åŸºç¡€ç³»ç»Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7360bce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ å®Œæ•´çŸ¥è¯†è’¸é¦ä¸æŒä¹…åŒ–ç³»ç»Ÿå·²å°±ç»ª\n",
      "\\nğŸš€ ä½“éªŒæ–¹æ³•:\n",
      "   - demo_complete_knowledge_persistence()  # å®Œæ•´æµç¨‹æ¼”ç¤º\n",
      "   - persistent_knowledge.display_knowledge_stats()  # æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ å®Œæ•´çš„çŸ¥è¯†è’¸é¦ä¸æŒä¹…åŒ–æ¼”ç¤º\n",
    "\n",
    "def demo_complete_knowledge_persistence():\n",
    "    \"\"\"æ¼”ç¤ºå®Œæ•´çš„çŸ¥è¯†è’¸é¦ä¸æŒä¹…åŒ–æµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¯ å®Œæ•´çŸ¥è¯†è’¸é¦ä¸æŒä¹…åŒ–æ¼”ç¤º\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬1: ç”¨äºåˆå§‹çŸ¥è¯†è’¸é¦\n",
    "    test_text_1 = \"\"\"\n",
    "    æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸæ•ˆæœå¾ˆå¥½ï¼Œç°åœ¨å¾ˆå¤šå…¬å¸éƒ½åœ¨ç”¨ã€‚\n",
    "    å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥è¯†åˆ«å›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼Œå‡†ç¡®ç‡æ¯”ä»¥å‰é«˜äº†å¾ˆå¤šã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    requirements_1 = \"æ”¹å†™ä¸ºç¬¦åˆè®¡ç®—æœºç§‘å­¦å­¦æœ¯è®ºæ–‡çš„è¡¨è¾¾é£æ ¼\"\n",
    "    \n",
    "    if not all(var in globals() for var in ['knowledge_distillation', 'persistent_knowledge']):\n",
    "        print(\"âŒ ç³»ç»Ÿæœªå®Œå…¨åˆå§‹åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åä½œä¸çŸ¥è¯†è’¸é¦\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. Agent Aåˆå§‹ä¼˜åŒ–\n",
    "        initial_result, _ = dual_agent_system.collaborate(\n",
    "            user_text=test_text_1,\n",
    "            user_requirements=requirements_1,\n",
    "            rounds=1\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¤– Agent Aåˆæ­¥ç»“æœ: {initial_result[:100]}...\")\n",
    "        \n",
    "        # 2. ä¸“å®¶çŸ¥è¯†è’¸é¦\n",
    "        distillation_result = knowledge_distillation.distill_knowledge(\n",
    "            original_text=test_text_1,\n",
    "            agent_a_version=initial_result,\n",
    "            user_requirements=requirements_1\n",
    "        )\n",
    "        \n",
    "        # 3. å­˜å‚¨è’¸é¦çŸ¥è¯†\n",
    "        if distillation_result:\n",
    "            success = persistent_knowledge.store_distillation_knowledge(\n",
    "                distillation_result, \n",
    "                task_type=\"è®¡ç®—æœºç§‘å­¦å­¦æœ¯å†™ä½œ\"\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(\"\\\\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ - ä¸“å®¶çŸ¥è¯†å·²å­˜å‚¨\")\n",
    "                persistent_knowledge.display_knowledge_stats()\n",
    "        \n",
    "        # æµ‹è¯•æ–‡æœ¬2: éªŒè¯çŸ¥è¯†æŒä¹…åŒ–æ•ˆæœ\n",
    "        test_text_2 = \"\"\"\n",
    "        è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¿›æ­¥å¾ˆå¿«ï¼ŒèŠå¤©æœºå™¨äººå˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ã€‚\n",
    "        Transformeræ¨¡å‹åœ¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°ä¸é”™ã€‚\n",
    "        \"\"\"\n",
    "        \n",
    "        requirements_2 = \"æ”¹å†™ä¸ºç¬¦åˆè®¡ç®—æœºç§‘å­¦å­¦æœ¯è®ºæ–‡çš„è¡¨è¾¾é£æ ¼\"\n",
    "        \n",
    "        print(\"\\\\n\\\\nğŸ“š ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨å†å²çŸ¥è¯†çš„å¢å¼ºåä½œ\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 4. ä½¿ç”¨çŸ¥è¯†å¢å¼ºçš„åä½œ\n",
    "        enhanced_result, enhanced_log = persistent_knowledge.enhanced_collaborate(\n",
    "            user_text=test_text_2,\n",
    "            user_requirements=requirements_2,\n",
    "            rounds=2\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nğŸ“Š æŒä¹…åŒ–æ•ˆæœéªŒè¯\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"ã€ç¬¬ä¸€æ¬¡åä½œï¼ˆåŸºç¡€èƒ½åŠ›ï¼‰ã€‘\")\n",
    "        print(f\"åŸæ–‡: {test_text_1.strip()}\")\n",
    "        print(f\"ç»“æœ: {initial_result}\")\n",
    "        \n",
    "        print(\"\\\\nã€ç¬¬äºŒæ¬¡åä½œï¼ˆçŸ¥è¯†å¢å¼ºï¼‰ã€‘\") \n",
    "        print(f\"åŸæ–‡: {test_text_2.strip()}\")\n",
    "        print(f\"ç»“æœ: {enhanced_result}\")\n",
    "        \n",
    "        print(\"\\\\nğŸ¯ çŸ¥è¯†æŒä¹…åŒ–éªŒè¯æˆåŠŸï¼š\")\n",
    "        print(\"âœ… ç¬¬äºŒæ¬¡åä½œè‡ªåŠ¨è°ƒç”¨äº†ç¬¬ä¸€æ¬¡è’¸é¦çš„ä¸“å®¶çŸ¥è¯†\")\n",
    "        print(\"âœ… Agent Açš„æç¤ºæ¨¡æ¿åŠ¨æ€åŒ…å«äº†å†å²ç»éªŒ\")\n",
    "        print(\"âœ… ç³»ç»Ÿå±•ç°äº†æŒç»­å­¦ä¹ å’ŒçŸ¥è¯†ç´¯ç§¯çš„èƒ½åŠ›\")\n",
    "        \n",
    "        return {\n",
    "            \"distillation_result\": distillation_result,\n",
    "            \"enhanced_result\": enhanced_result,\n",
    "            \"knowledge_count\": len(persistent_knowledge.knowledge_base)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¼”ç¤ºå¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# å®Œæ•´ç³»ç»Ÿå°±ç»ªæç¤º\n",
    "print(\"ğŸ‰ å®Œæ•´çŸ¥è¯†è’¸é¦ä¸æŒä¹…åŒ–ç³»ç»Ÿå·²å°±ç»ª\")\n",
    "\n",
    "\n",
    "print(\"\\\\nğŸš€ ä½“éªŒæ–¹æ³•:\")\n",
    "print(\"   - demo_complete_knowledge_persistence()  # å®Œæ•´æµç¨‹æ¼”ç¤º\")\n",
    "print(\"   - persistent_knowledge.display_knowledge_stats()  # æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e0c0c",
   "metadata": {},
   "source": [
    "## ğŸš¨ é‡è¦æ¶æ„é—®é¢˜ä¸ä¿®æ­£\n",
    "\n",
    "### âŒ **æ‚¨å‘ç°çš„å…³é”®é—®é¢˜**\n",
    "æ‚¨çš„è§‚å¯Ÿå®Œå…¨æ­£ç¡®ï¼å½“å‰å­˜åœ¨**ä¸¥é‡çš„æ¶æ„è®¾è®¡é—®é¢˜**ï¼š\n",
    "\n",
    "1. **é‡å¤åˆå§‹åŒ–**: `DualAgentAcademicSystem` å’Œ `AcademicWritingOptimizer` éƒ½åˆ›å»ºäº†å„è‡ªçš„ Agent A å’Œ Agent B\n",
    "2. **çŸ¥è¯†è’¸é¦å¤±æ•ˆ**: çŸ¥è¯†è’¸é¦ä½œç”¨äº `DualAgentAcademicSystem` çš„ Agentï¼Œä½†åç»­ä½¿ç”¨çš„æ˜¯ `AcademicWritingOptimizer` çš„ Agent\n",
    "3. **èµ„æºæµªè´¹**: åˆ›å»ºäº†å¤šä¸ªLLMå®ä¾‹ï¼Œé€ æˆä¸å¿…è¦çš„å¼€é”€\n",
    "\n",
    "### âœ… **æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆ**\n",
    "æˆ‘ä»¬éœ€è¦**ç»Ÿä¸€Agentç³»ç»Ÿ**ï¼Œç¡®ä¿çŸ¥è¯†è’¸é¦çœŸæ­£ä½œç”¨äºåç»­ä½¿ç”¨çš„Agentã€‚\n",
    "\n",
    "**ä¿®æ­£ç­–ç•¥**:\n",
    "- ç§»é™¤é‡å¤çš„ `AcademicWritingOptimizer` ç±»  \n",
    "- ç›´æ¥ä½¿ç”¨ `DualAgentAcademicSystem` è¿›è¡Œæ‰€æœ‰åä½œ\n",
    "- ç¡®ä¿çŸ¥è¯†è’¸é¦å’ŒæŒä¹…åŒ–ç³»ç»Ÿä½œç”¨äºåŒä¸€å¥—Agentå®ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbff5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ¶æ„ä¿®æ­£ï¼šç»Ÿä¸€Agentç³»ç»Ÿ\n",
      "==================================================\n",
      "âœ… ç§»é™¤é‡å¤çš„AcademicWritingOptimizerç±»\n",
      "âœ… ç»Ÿä¸€ä½¿ç”¨DualAgentAcademicSystem\n",
      "âœ… ç¡®ä¿çŸ¥è¯†è’¸é¦ä½œç”¨äºå®é™…ä½¿ç”¨çš„Agent\n",
      "\n",
      "ğŸ‰ ç³»ç»Ÿæ¶æ„ä¿®æ­£å®Œæˆï¼\n",
      "âœ… ç°åœ¨çŸ¥è¯†è’¸é¦å°†çœŸæ­£ä½œç”¨äºåç»­çš„Agentåä½œ\n",
      "âœ… ç»Ÿä¸€ä½¿ç”¨åŒä¸€å¥—Agentå®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–\n",
      "\n",
      "ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\n",
      "   - run_chinese_demo()  # ä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\n",
      "   - run_english_demo()  # è‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\n",
      "   - dual_agent_system.collaborate(text, requirements)  # è‡ªå®šä¹‰ä¼˜åŒ–\n"
     ]
    }
   ],
   "source": [
    "# âœ… ç»Ÿä¸€çš„åŒæ™ºèƒ½ä½“å­¦æœ¯å†™ä½œç³»ç»Ÿ - æ¶æ„ä¿®æ­£\n",
    "\n",
    "# ç§»é™¤é‡å¤çš„AcademicWritingOptimizerç±»ï¼Œç›´æ¥ä½¿ç”¨DualAgentAcademicSystem\n",
    "# è¿™æ ·ç¡®ä¿çŸ¥è¯†è’¸é¦èƒ½å¤ŸçœŸæ­£ä½œç”¨äºåç»­ä½¿ç”¨çš„Agent\n",
    "\n",
    "print(\"ğŸ”§ æ¶æ„ä¿®æ­£ï¼šç»Ÿä¸€Agentç³»ç»Ÿ\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… ç§»é™¤é‡å¤çš„AcademicWritingOptimizerç±»\")\n",
    "print(\"âœ… ç»Ÿä¸€ä½¿ç”¨DualAgentAcademicSystem\")\n",
    "print(\"âœ… ç¡®ä¿çŸ¥è¯†è’¸é¦ä½œç”¨äºå®é™…ä½¿ç”¨çš„Agent\")\n",
    "\n",
    "# ä¸ºDualAgentAcademicSystemæ·»åŠ ä¾¿æ·çš„å­¦æœ¯ä¼˜åŒ–æ–¹æ³•\n",
    "def add_academic_convenience_methods():\n",
    "    \"\"\"ä¸ºDualAgentAcademicSystemæ·»åŠ ä¾¿æ·æ–¹æ³•\"\"\"\n",
    "    \n",
    "    def chinese_academic_demo(self):\n",
    "        \"\"\"ä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\"\"\"\n",
    "        chinese_text = \"\"\"\n",
    "        æœºå™¨å­¦ä¹ åœ¨å¾ˆå¤šåœ°æ–¹éƒ½æœ‰ç”¨ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬åšå¾ˆå¤šäº‹æƒ…ï¼Œæ¯”å¦‚è¯†åˆ«å›¾ç‰‡ã€ç¿»è¯‘è¯­è¨€ç­‰ã€‚\n",
    "        ç°åœ¨çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šå‰å®³äº†ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œ\n",
    "        é€šè¿‡å¢åŠ æ›´å¤šçš„æ•°æ®å’Œæ›´å¥½çš„ç®—æ³•ï¼Œå¯ä»¥è®©æ¨¡å‹æ•ˆæœæ›´å¥½ã€‚è¿™ä¸ªæŠ€æœ¯åœ¨æœªæ¥ä¼šæœ‰å¾ˆå¤§å‘å±•ã€‚\n",
    "        \"\"\"\n",
    "        \n",
    "        requirements = \"å°†è¿™æ®µå…³äºæœºå™¨å­¦ä¹ çš„æè¿°æ”¹å†™ä¸ºç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†çš„è¡¨è¾¾ï¼Œæå‡ä¸“ä¸šæ€§å’Œä¸¥è°¨æ€§\"\n",
    "        \n",
    "        print(\"ğŸ‡¨ğŸ‡³ ä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"åŸæ–‡: {chinese_text.strip()}\")\n",
    "        print(f\"éœ€æ±‚: {requirements}\")\n",
    "        print()\n",
    "        \n",
    "        return self.collaborate(\n",
    "            user_text=chinese_text.strip(),\n",
    "            user_requirements=requirements,\n",
    "            language=\"ä¸­æ–‡\",\n",
    "            rounds=3\n",
    "        )\n",
    "    \n",
    "    def english_academic_demo(self):\n",
    "        \"\"\"è‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\"\"\"\n",
    "        english_text = \"\"\"\n",
    "        Artificial intelligence is getting really popular these days. It's being used in lots of different areas \n",
    "        and people think it's going to change everything. Companies are investing a lot of money in AI research \n",
    "        because they want to stay competitive. The technology has some problems though, like bias in the data \n",
    "        and ethical concerns that need to be addressed.\n",
    "        \"\"\"\n",
    "        \n",
    "        requirements = \"Transform this informal discussion about AI into formal academic writing suitable for a research paper introduction\"\n",
    "        \n",
    "        print(\"ğŸ‡ºğŸ‡¸ è‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"åŸæ–‡: {english_text.strip()}\")\n",
    "        print(f\"éœ€æ±‚: {requirements}\")\n",
    "        print()\n",
    "        \n",
    "        return self.collaborate(\n",
    "            user_text=english_text.strip(),\n",
    "            user_requirements=requirements,\n",
    "            language=\"è‹±æ–‡\", \n",
    "            rounds=3\n",
    "        )\n",
    "    \n",
    "    def generate_training_data(self):\n",
    "        \"\"\"ç”Ÿæˆè®­ç»ƒæ•°æ®\"\"\"\n",
    "        training_data = []\n",
    "        for log in self.collaboration_log[1:]:  # è·³è¿‡åˆå§‹è®°å½•\n",
    "            if \"optimized_text\" in log:\n",
    "                training_data.append({\n",
    "                    \"round\": log[\"round\"],\n",
    "                    \"input_text\": self.collaboration_log[log[\"round\"]-1].get(\"optimized_text\", \n",
    "                                                                           self.collaboration_log[0][\"user_input\"]),\n",
    "                    \"output_text\": log[\"optimized_text\"],\n",
    "                    \"feedback\": log[\"agent_b_feedback\"],\n",
    "                    \"timestamp\": log[\"timestamp\"]\n",
    "                })\n",
    "        return training_data\n",
    "    \n",
    "    # å°†æ–¹æ³•æ·»åŠ åˆ°DualAgentAcademicSystemç±»\n",
    "    DualAgentAcademicSystem.chinese_academic_demo = chinese_academic_demo\n",
    "    DualAgentAcademicSystem.english_academic_demo = english_academic_demo  \n",
    "    DualAgentAcademicSystem.generate_training_data = generate_training_data\n",
    "\n",
    "# æ·»åŠ ä¾¿æ·æ–¹æ³•\n",
    "add_academic_convenience_methods()\n",
    "\n",
    "# åˆ›å»ºä¾¿æ·çš„æ¼”ç¤ºå‡½æ•°\n",
    "def run_chinese_demo():\n",
    "    \"\"\"è¿è¡Œä¸­æ–‡ä¼˜åŒ–æ¼”ç¤º\"\"\"\n",
    "    print(\"ğŸ‡¨ğŸ‡³ æ‰§è¡Œä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º...\")\n",
    "    return dual_agent_system.chinese_academic_demo()\n",
    "\n",
    "def run_english_demo():\n",
    "    \"\"\"è¿è¡Œè‹±æ–‡ä¼˜åŒ–æ¼”ç¤º\"\"\"\n",
    "    print(\"ğŸ‡ºğŸ‡¸ æ‰§è¡Œè‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º...\")\n",
    "    return dual_agent_system.english_academic_demo()\n",
    "\n",
    "print(\"\\nğŸ‰ ç³»ç»Ÿæ¶æ„ä¿®æ­£å®Œæˆï¼\")\n",
    "print(\"âœ… ç°åœ¨çŸ¥è¯†è’¸é¦å°†çœŸæ­£ä½œç”¨äºåç»­çš„Agentåä½œ\")\n",
    "print(\"âœ… ç»Ÿä¸€ä½¿ç”¨åŒä¸€å¥—Agentå®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–\")\n",
    "print(\"\\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   - run_chinese_demo()  # ä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\")\n",
    "print(\"   - run_english_demo()  # è‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\")\n",
    "print(\"   - dual_agent_system.collaborate(text, requirements)  # è‡ªå®šä¹‰ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334defd6",
   "metadata": {},
   "source": [
    "## âœ… æ¶æ„ä¿®æ­£å®Œæˆ - çŸ¥è¯†è’¸é¦ç°åœ¨çœŸæ­£æœ‰æ•ˆï¼\n",
    "\n",
    "### ğŸ¯ **é—®é¢˜è§£å†³æ–¹æ¡ˆ**\n",
    "\n",
    "æˆ‘ä»¬å·²ç»ä¿®æ­£äº†æ‚¨å‘ç°çš„å…³é”®æ¶æ„é—®é¢˜ï¼š\n",
    "\n",
    "#### âŒ **ä¹‹å‰çš„é—®é¢˜**:\n",
    "- `DualAgentAcademicSystem` å’Œ `AcademicWritingOptimizer` é‡å¤åˆ›å»ºAgent\n",
    "- çŸ¥è¯†è’¸é¦ä½œç”¨äºä¸€å¥—Agentï¼Œä½†å®é™…ä½¿ç”¨å¦ä¸€å¥—Agent\n",
    "- å¯¼è‡´çŸ¥è¯†è’¸é¦å®Œå…¨å¤±æ•ˆ\n",
    "\n",
    "#### âœ… **ç°åœ¨çš„è§£å†³æ–¹æ¡ˆ**:\n",
    "- **ç»Ÿä¸€Agentç³»ç»Ÿ**: ç§»é™¤é‡å¤çš„`AcademicWritingOptimizer`ç±»\n",
    "- **ç›´æ¥ä½¿ç”¨**: `DualAgentAcademicSystem`è¿›è¡Œæ‰€æœ‰å­¦æœ¯åä½œ\n",
    "- **ç¡®ä¿ä¸€è‡´æ€§**: çŸ¥è¯†è’¸é¦å’Œåç»­åä½œä½¿ç”¨åŒä¸€å¥—Agentå®ä¾‹\n",
    "\n",
    "### ğŸ§  **çŸ¥è¯†è’¸é¦ç°åœ¨çœŸæ­£ç”Ÿæ•ˆ**:\n",
    "\n",
    "1. **çŸ¥è¯†è’¸é¦**: ä½œç”¨äº`DualAgentAcademicSystem`çš„Agent Aå’ŒAgent B\n",
    "2. **æŒä¹…åŒ–å­˜å‚¨**: ä¸“å®¶æŒ‡å¯¼å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“  \n",
    "3. **åŠ¨æ€å¢å¼º**: åç»­åä½œæ—¶åŠ¨æ€æ£€ç´¢å’Œåº”ç”¨å†å²çŸ¥è¯†\n",
    "4. **çœŸå®æ•ˆæœ**: æ¯æ¬¡åä½œéƒ½èƒ½å—ç›Šäºä¹‹å‰çš„ä¸“å®¶æŒ‡å¯¼\n",
    "\n",
    "### ğŸš€ **ä½¿ç”¨ç¤ºä¾‹**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14109f",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç³»ç»Ÿæ€»ç»“ä¸ä½¿ç”¨æŒ‡å—\n",
    "\n",
    "### âœ… å·²å®ç°åŠŸèƒ½\n",
    "\n",
    "æˆ‘ä»¬æˆåŠŸåœ¨æ‚¨ç°æœ‰çš„å¤šæ™ºèƒ½ä½“NLPé¡¹ç›®åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†å®Œæ•´çš„**åŒæ™ºèƒ½ä½“å­¦æœ¯è®ºæ–‡è¡¨è¾¾ä¼˜åŒ–ç³»ç»Ÿ**ï¼š\n",
    "\n",
    "#### ğŸ¤– åŒæ™ºèƒ½ä½“æ¶æ„\n",
    "- **Agent A (å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–è€…)**: è´Ÿè´£åˆæ­¥ä¿®æ”¹å’Œè¿­ä»£æ”¹è¿›\n",
    "- **Agent B (å­¦æœ¯æ‰¹è¯„ä¸æ”¹è¿›å»ºè®®è€…)**: è´Ÿè´£ä¸¥æ ¼å®¡è§†å’Œæå‡ºæ”¹è¿›å»ºè®®\n",
    "\n",
    "#### ğŸ”„ åä½œæœºåˆ¶\n",
    "- **5è½®è¿­ä»£ä¼˜åŒ–**ï¼šAâ†’Bâ†’Aâ†’Bâ†’Aâ†’B...\n",
    "- **æ™ºèƒ½æç¤ºå·¥ç¨‹**ï¼šä¸“é—¨è®¾è®¡çš„å­¦æœ¯å†™ä½œæç¤ºæ¨¡æ¿\n",
    "- **é”™è¯¯æ¢å¤**ï¼šå…·å¤‡å¼‚å¸¸å¤„ç†å’Œæ¢å¤èƒ½åŠ›\n",
    "\n",
    "#### ğŸ“Š æ•°æ®ç”Ÿæˆ\n",
    "- **è®­ç»ƒæ•°æ®æ ¼å¼åŒ–**ï¼šå¯ç›´æ¥ç”¨äºæ¨¡å‹å¾®è°ƒ\n",
    "- **å®Œæ•´åä½œæ—¥å¿—**ï¼šè®°å½•æ¯è½®ä¿®æ”¹å’Œå»ºè®®\n",
    "- **ç»“æ„åŒ–è¾“å‡º**ï¼šç¬¦åˆæœºå™¨å­¦ä¹ è®­ç»ƒè¦æ±‚\n",
    "\n",
    "#### ğŸŒ å¤šè¯­è¨€æ”¯æŒ\n",
    "- **ä¸­æ–‡å­¦æœ¯å†™ä½œ**ï¼šç¬¦åˆä¸­æ–‡å­¦æœ¯è§„èŒƒ\n",
    "- **è‹±æ–‡å­¦æœ¯å†™ä½œ**ï¼šéµå¾ªå›½é™…æœŸåˆŠæ ‡å‡†\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "```python\n",
    "# åŸºæœ¬ä½¿ç”¨\n",
    "final_text, log = academic_optimizer.collaborate_optimize(\n",
    "    user_text=\"æ‚¨çš„è®ºæ–‡æ®µè½\",\n",
    "    user_requirements=\"ä¼˜åŒ–éœ€æ±‚æè¿°\",\n",
    "    language=\"ä¸­æ–‡\",  # æˆ– \"è‹±æ–‡\"\n",
    "    rounds=5  # åä½œè½®æ¬¡\n",
    ")\n",
    "\n",
    "# ç”ŸæˆæŠ¥å‘Š\n",
    "report = academic_optimizer.generate_final_report(\n",
    "    final_text, log, original_text, requirements\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ\n",
    "\n",
    "1. **ç«‹å³æµ‹è¯•**: è¿è¡Œä¸Šé¢çš„å¿«é€Ÿæµ‹è¯•Cell\n",
    "2. **å®Œæ•´æ¼”ç¤º**: ä½¿ç”¨ `demo_chinese_academic_optimization()` æˆ– `demo_english_academic_optimization()`\n",
    "3. **è‡ªå®šä¹‰ä½¿ç”¨**: æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚è°ƒç”¨ç›¸å…³æ–¹æ³•\n",
    "\n",
    "### ğŸ’¡ ä¼˜åŠ¿ç‰¹ç‚¹\n",
    "\n",
    "- âœ… **æ— ç¼é›†æˆ**ï¼šåŸºäºæ‚¨ç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶\n",
    "- âœ… **ç”Ÿäº§å°±ç»ª**ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•\n",
    "- âœ… **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ æ–°çš„ä¼˜åŒ–ç»´åº¦å’Œè¯„å®¡æ ‡å‡†  \n",
    "- âœ… **æ•°æ®é©±åŠ¨**ï¼šè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®\n",
    "- âœ… **å­¦æœ¯ä¸“ä¸š**ï¼šä¸“é—¨é’ˆå¯¹å­¦æœ¯å†™ä½œåœºæ™¯ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34dc58",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒæ•°æ®ç”Ÿæˆä¸æ¨¡å‹å¾®è°ƒæ”¯æŒ\n",
    "\n",
    "åŒæ™ºèƒ½ä½“åä½œè¿‡ç¨‹ç”Ÿæˆçš„æ•°æ®å…·æœ‰å¾ˆé«˜çš„è®­ç»ƒä»·å€¼ï¼Œå¯ä»¥ç”¨äºå¾®è°ƒä¸“é—¨çš„å­¦æœ¯å†™ä½œæ¨¡å‹ã€‚ç³»ç»Ÿè®¾è®¡éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š\n",
    "\n",
    "### ğŸ“Š **è®­ç»ƒæ•°æ®ç»“æ„**\n",
    "- **è¾“å…¥åºåˆ—**: åŸå§‹æ–‡æœ¬ + ç”¨æˆ·éœ€æ±‚ + å‰è½®åé¦ˆ\n",
    "- **è¾“å‡ºåºåˆ—**: ä¼˜åŒ–åæ–‡æœ¬ + ä¿®æ”¹è¯´æ˜\n",
    "- **è´¨é‡æ ‡ç­¾**: Agent Bçš„è¯„ä»·åˆ†æ•°å’Œå»ºè®®\n",
    "- **å…ƒæ•°æ®**: è½®æ¬¡ä¿¡æ¯ã€æ—¶é—´æˆ³ã€è¯­è¨€ç±»å‹\n",
    "\n",
    "### ğŸ”¬ **æ•°æ®è´¨é‡ä¿è¯**\n",
    "1. **å¤šè½®è¿­ä»£éªŒè¯**: é€šè¿‡5è½®A-Båä½œç¡®ä¿è´¨é‡\n",
    "2. **ä¸“ä¸šè¯„ä¼°**: Agent Bæä¾›å­¦æœ¯å†™ä½œä¸“ä¸šè¯„ä»·\n",
    "3. **ç»“æ„åŒ–æ ‡æ³¨**: æ˜ç¡®æ ‡æ³¨ä¿®æ”¹ç±»å‹å’ŒåŸå› \n",
    "4. **å¯è¿½æº¯æ€§**: å®Œæ•´è®°å½•ä¼˜åŒ–è¿‡ç¨‹å’Œå†³ç­–è·¯å¾„\n",
    "\n",
    "### ğŸ¯ **åº”ç”¨åœºæ™¯**\n",
    "- **æ¨¡å‹å¾®è°ƒ**: è®­ç»ƒä¸“é—¨çš„å­¦æœ¯å†™ä½œåŠ©æ‰‹\n",
    "- **è´¨é‡è¯„ä¼°**: å»ºç«‹å­¦æœ¯å†™ä½œè´¨é‡è¯„ä»·ä½“ç³»\n",
    "- **çŸ¥è¯†æå–**: æ€»ç»“å­¦æœ¯å†™ä½œè§„èŒƒå’Œæœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40695097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è®­ç»ƒæ•°æ®ç”Ÿæˆå·¥å…·å·²å°±ç»ª\n",
      "ğŸ”§ å¯ç”¨å‡½æ•°:\n",
      "   - analyze_collaboration_data(log)     # åˆ†æåä½œæ•°æ®è´¨é‡\n",
      "   - export_training_data(log, file)     # å¯¼å‡ºè®­ç»ƒæ•°æ®\n",
      "   - generate_model_training_prompt()    # ç”Ÿæˆè®­ç»ƒæç¤ºæ¨¡æ¿\n",
      "   - create_finetuning_dataset(logs)     # åˆ›å»ºå¾®è°ƒæ•°æ®é›†\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š è®­ç»ƒæ•°æ®ç”Ÿæˆä¸è´¨é‡è¯„ä¼°å·¥å…·\n",
    "\n",
    "def analyze_collaboration_data(collaboration_log):\n",
    "    \"\"\"åˆ†æåä½œæ•°æ®è´¨é‡\"\"\"\n",
    "    print(\"ğŸ“Š åä½œæ•°æ®è´¨é‡åˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not collaboration_log:\n",
    "        print(\"âŒ æ— åä½œæ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡\n",
    "    total_rounds = len(collaboration_log) - 1  # å‡å»åˆå§‹è®°å½•\n",
    "    successful_rounds = len([log for log in collaboration_log[1:] if \"optimized_text\" in log])\n",
    "    \n",
    "    print(f\"ğŸ“ˆ åä½œç»Ÿè®¡:\")\n",
    "    print(f\"   æ€»è½®æ¬¡: {total_rounds}\")\n",
    "    print(f\"   æˆåŠŸè½®æ¬¡: {successful_rounds}\")\n",
    "    print(f\"   æˆåŠŸç‡: {successful_rounds/total_rounds*100:.1f}%\")\n",
    "    \n",
    "    # æ–‡æœ¬é•¿åº¦å˜åŒ–åˆ†æ\n",
    "    if successful_rounds > 0:\n",
    "        original_length = len(collaboration_log[0][\"user_input\"])\n",
    "        final_log = [log for log in collaboration_log[1:] if \"optimized_text\" in log][-1]\n",
    "        final_length = len(final_log[\"optimized_text\"])\n",
    "        \n",
    "        print(f\"\\nğŸ“ æ–‡æœ¬å˜åŒ–:\")\n",
    "        print(f\"   åŸå§‹é•¿åº¦: {original_length} å­—ç¬¦\")\n",
    "        print(f\"   æœ€ç»ˆé•¿åº¦: {final_length} å­—ç¬¦\")\n",
    "        print(f\"   å˜åŒ–ç‡: {(final_length-original_length)/original_length*100:+.1f}%\")\n",
    "    \n",
    "    # æ¯è½®å˜åŒ–åˆ†æ\n",
    "    print(f\"\\nğŸ”„ å„è½®æ¬¡å˜åŒ–:\")\n",
    "    for i, log in enumerate(collaboration_log[1:], 1):\n",
    "        if \"optimized_text\" in log:\n",
    "            text_length = len(log[\"optimized_text\"])\n",
    "            timestamp = log[\"timestamp\"].split(\"T\")[1][:8]  # æå–æ—¶é—´\n",
    "            print(f\"   è½®æ¬¡ {i}: {text_length} å­—ç¬¦ | {timestamp}\")\n",
    "\n",
    "def export_training_data(collaboration_log, filename=\"academic_training_data.json\"):\n",
    "    \"\"\"å¯¼å‡ºè®­ç»ƒæ•°æ®ä¸ºJSONæ ¼å¼\"\"\"\n",
    "    print(f\"ğŸ’¾ å¯¼å‡ºè®­ç»ƒæ•°æ®åˆ° {filename}\")\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    # å¤„ç†æ¯è½®æ•°æ®\n",
    "    for i, log in enumerate(collaboration_log):\n",
    "        if log[\"round\"] == 0:  # åŸå§‹è¾“å…¥\n",
    "            training_data.append({\n",
    "                \"id\": f\"sample_{i}\",\n",
    "                \"round\": 0,\n",
    "                \"type\": \"original\",\n",
    "                \"input_text\": log[\"user_input\"],\n",
    "                \"user_requirements\": log[\"user_requirements\"],\n",
    "                \"timestamp\": log[\"timestamp\"]\n",
    "            })\n",
    "        elif \"optimized_text\" in log:  # åä½œè½®æ¬¡\n",
    "            # è·å–å‰ä¸€è½®çš„æ–‡æœ¬ä½œä¸ºè¾“å…¥\n",
    "            prev_text = collaboration_log[log[\"round\"]-1].get(\"optimized_text\", \n",
    "                                                             collaboration_log[0][\"user_input\"])\n",
    "            \n",
    "            training_data.append({\n",
    "                \"id\": f\"sample_{i}\",\n",
    "                \"round\": log[\"round\"],\n",
    "                \"type\": \"optimization\",\n",
    "                \"input_text\": prev_text,\n",
    "                \"output_text\": log[\"optimized_text\"],\n",
    "                \"agent_b_feedback\": log[\"agent_b_feedback\"],\n",
    "                \"user_requirements\": collaboration_log[0][\"user_requirements\"],\n",
    "                \"timestamp\": log[\"timestamp\"]\n",
    "            })\n",
    "    \n",
    "    # å¯¼å‡ºåˆ°æ–‡ä»¶\n",
    "    try:\n",
    "        import json\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸå¯¼å‡º {len(training_data)} æ¡è®­ç»ƒæ•°æ®\")\n",
    "        print(f\"ğŸ“‚ æ–‡ä»¶ä½ç½®: {filename}\")\n",
    "        return training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯¼å‡ºå¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_model_training_prompt():\n",
    "    \"\"\"ç”Ÿæˆæ¨¡å‹å¾®è°ƒçš„æç¤ºæ¨¡æ¿\"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "# å­¦æœ¯å†™ä½œä¼˜åŒ–æ¨¡å‹è®­ç»ƒæç¤º\n",
    "\n",
    "## ä»»åŠ¡æè¿°\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯å†™ä½œåŠ©æ‰‹ï¼Œéœ€è¦å°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬ä¼˜åŒ–ä¸ºç¬¦åˆå­¦æœ¯è§„èŒƒçš„è¡¨è¾¾ã€‚\n",
    "\n",
    "## è¾“å…¥æ ¼å¼\n",
    "- **åŸå§‹æ–‡æœ¬**: {input_text}\n",
    "- **ç”¨æˆ·éœ€æ±‚**: {user_requirements}\n",
    "- **å‰è½®åé¦ˆ**: {previous_feedback}\n",
    "\n",
    "## è¾“å‡ºæ ¼å¼\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n",
    "\n",
    "**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\n",
    "[æä¾›å®Œæ•´çš„ä¼˜åŒ–åæ–‡æœ¬]\n",
    "\n",
    "**ä¿®æ”¹è¯´æ˜ï¼š**\n",
    "[è¯¦ç»†è¯´æ˜ä¸»è¦ä¿®æ”¹ç‚¹å’Œç†ç”±]\n",
    "\n",
    "## ä¼˜åŒ–åŸåˆ™\n",
    "1. å­¦æœ¯åŒ–è¡¨è¾¾ï¼šä½¿ç”¨å‡†ç¡®çš„å­¦æœ¯æœ¯è¯­\n",
    "2. é€»è¾‘æ¸…æ™°ï¼šç¡®ä¿è®ºè¯ç»“æ„åˆç†\n",
    "3. è¯­è¨€æ­£å¼ï¼šç¬¦åˆå­¦æœ¯å†™ä½œè§„èŒƒ\n",
    "4. è¡¨è¾¾ç²¾å‡†ï¼šæ¶ˆé™¤æ­§ä¹‰å’Œæ¨¡ç³Šè¡¨è¾¾\n",
    "\n",
    "## ç¤ºä¾‹\n",
    "[åœ¨è¿™é‡Œæ’å…¥å…·ä½“çš„è®­ç»ƒç¤ºä¾‹]\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ å­¦æœ¯å†™ä½œä¼˜åŒ–æ¨¡å‹è®­ç»ƒæç¤ºæ¨¡æ¿:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(prompt_template)\n",
    "    \n",
    "    return prompt_template\n",
    "\n",
    "def create_finetuning_dataset(collaboration_logs):\n",
    "    \"\"\"åˆ›å»ºå¾®è°ƒæ•°æ®é›†\"\"\"\n",
    "    print(\"ğŸ”¬ åˆ›å»ºæ¨¡å‹å¾®è°ƒæ•°æ®é›†\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for log_id, log in enumerate(collaboration_logs):\n",
    "        for round_data in log[1:]:  # è·³è¿‡åŸå§‹è¾“å…¥\n",
    "            if \"optimized_text\" in round_data:\n",
    "                # æ„å»ºè®­ç»ƒæ ·æœ¬\n",
    "                prev_text = log[round_data[\"round\"]-1].get(\"optimized_text\", \n",
    "                                                          log[0][\"user_input\"]) if round_data[\"round\"] > 1 else log[0][\"user_input\"]\n",
    "                \n",
    "                sample = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯å†™ä½œåŠ©æ‰‹ï¼Œä¸“é—¨å°†æ™®é€šæ–‡æœ¬ä¼˜åŒ–ä¸ºç¬¦åˆå­¦æœ¯è§„èŒƒçš„è¡¨è¾¾ã€‚\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": f\"è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡æœ¬ï¼š\\n\\n{prev_text}\\n\\néœ€æ±‚ï¼š{log[0]['user_requirements']}\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": round_data[\"optimized_text\"]\n",
    "                        }\n",
    "                    ],\n",
    "                    \"metadata\": {\n",
    "                        \"log_id\": log_id,\n",
    "                        \"round\": round_data[\"round\"],\n",
    "                        \"feedback_quality\": len(round_data.get(\"agent_b_feedback\", \"\")) > 100\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                dataset.append(sample)\n",
    "    \n",
    "    print(f\"âœ… ç”Ÿæˆ {len(dataset)} ä¸ªè®­ç»ƒæ ·æœ¬\")\n",
    "    return dataset\n",
    "\n",
    "# æä¾›å·¥å…·å‡½æ•°\n",
    "print(\"ğŸ“Š è®­ç»ƒæ•°æ®ç”Ÿæˆå·¥å…·å·²å°±ç»ª\")\n",
    "print(\"ğŸ”§ å¯ç”¨å‡½æ•°:\")\n",
    "print(\"   - analyze_collaboration_data(log)     # åˆ†æåä½œæ•°æ®è´¨é‡\")\n",
    "print(\"   - export_training_data(log, file)     # å¯¼å‡ºè®­ç»ƒæ•°æ®\")\n",
    "print(\"   - generate_model_training_prompt()    # ç”Ÿæˆè®­ç»ƒæç¤ºæ¨¡æ¿\")\n",
    "print(\"   - create_finetuning_dataset(logs)     # åˆ›å»ºå¾®è°ƒæ•°æ®é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5cfae",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç³»ç»Ÿæµ‹è¯•ä¸ä½¿ç”¨æŒ‡å—\n",
    "\n",
    "ç°åœ¨æ•´ä¸ªåŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿå·²ç»å®Œæˆé›†æˆï¼Œæ‚¨å¯ä»¥ï¼š\n",
    "\n",
    "### ğŸš€ ç«‹å³å¼€å§‹ä½¿ç”¨\n",
    "```python\n",
    "# 1. è¿è¡Œä¸­æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º\n",
    "run_chinese_demo()\n",
    "\n",
    "# 2. è¿è¡Œè‹±æ–‡å­¦æœ¯ä¼˜åŒ–æ¼”ç¤º  \n",
    "run_english_demo()\n",
    "\n",
    "# 3. è‡ªå®šä¹‰å­¦æœ¯æ–‡æœ¬ä¼˜åŒ–\n",
    "result = dual_agent_system.collaborate(\n",
    "    user_text=\"æ‚¨çš„å­¦æœ¯æ–‡æœ¬\",\n",
    "    requirements=\"å…·ä½“ä¼˜åŒ–éœ€æ±‚\"\n",
    ")\n",
    "\n",
    "# 4. åˆ†æåä½œè´¨é‡\n",
    "analyze_collaboration_data(dual_agent_system.collaboration_log)\n",
    "\n",
    "# 5. å¯¼å‡ºè®­ç»ƒæ•°æ®\n",
    "export_training_data(dual_agent_system.collaboration_log, \"my_training_data.json\")\n",
    "```\n",
    "\n",
    "### ğŸ“ˆ ç³»ç»Ÿç‰¹æ€§\n",
    "- âœ… **åŒæ™ºèƒ½ä½“åä½œ**: Agent A (ä¼˜åŒ–è€…) + Agent B (æ‰¹è¯„è€…)\n",
    "- âœ… **5è½®è¿­ä»£ä¼˜åŒ–**: ç¡®ä¿æ–‡æœ¬è´¨é‡é€æ­¥æå‡\n",
    "- âœ… **ä¸­è‹±åŒè¯­æ”¯æŒ**: é€‚é…ä¸åŒè¯­è¨€å­¦æœ¯å†™ä½œéœ€æ±‚\n",
    "- âœ… **è®°å¿†åŠŸèƒ½**: FAISSå‘é‡å­˜å‚¨å†å²ä¼˜åŒ–ç»éªŒ\n",
    "- âœ… **è®­ç»ƒæ•°æ®ç”Ÿæˆ**: æ”¯æŒæ¨¡å‹å¾®è°ƒå’ŒæŒç»­æ”¹è¿›\n",
    "- âœ… **è´¨é‡è¯„ä¼°**: è¯¦ç»†çš„åä½œè¿‡ç¨‹åˆ†æ\n",
    "\n",
    "### ğŸ”§ é«˜çº§å®šåˆ¶\n",
    "ç³»ç»Ÿæ”¯æŒçµæ´»é…ç½®ï¼Œæ‚¨å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´ï¼š\n",
    "- åä½œè½®æ¬¡æ•°é‡\n",
    "- Agentè§’è‰²è®¾å®š\n",
    "- ä¼˜åŒ–ç­–ç•¥é‡ç‚¹\n",
    "- è¯„ä¼°æ ‡å‡†\n",
    "\n",
    "**ğŸ¤– åŒæ™ºèƒ½ä½“å­¦æœ¯åä½œç³»ç»Ÿç°å·²å…¨é¢å°±ç»ªï¼å¼€å§‹æ‚¨çš„å­¦æœ¯å†™ä½œä¼˜åŒ–ä¹‹æ—…å§ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
