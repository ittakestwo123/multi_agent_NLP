import os
from typing import Dict, Union, Optional

# å°è¯•å¯¼å…¥ torch ä¸ transformersï¼Œå¤±è´¥åˆ™æ ‡è®°ä¸å¯ç”¨å¹¶ä½¿ç”¨å ä½å®ç°
try:
    import torch  # type: ignore
    _TORCH_AVAILABLE = True
except Exception:
    torch = None  # type: ignore
    _TORCH_AVAILABLE = False

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM  # type: ignore
    _TRANSFORMERS_AVAILABLE = True
except Exception:
    AutoTokenizer = None  # type: ignore
    AutoModelForCausalLM = None  # type: ignore
    _TRANSFORMERS_AVAILABLE = False

try:
    from peft import PeftModel  # type: ignore
    _PEFT_AVAILABLE = True
except Exception:
    PeftModel = None  # type: ignore
    _PEFT_AVAILABLE = False

# å°è¯•å¯¼å…¥ bitsandbytes ä»¥æ”¯æŒ 4bit é‡åŒ–æ¨ç†
try:
    import bitsandbytes as bnb  # type: ignore  # noqa: F401
    _BNB_AVAILABLE = True
except Exception:
    _BNB_AVAILABLE = False

_FORCE_STUB = os.getenv("FORCE_STUDENT_STUB") == "1"
_NEED_STUB = _FORCE_STUB or (not _TORCH_AVAILABLE) or (not _TRANSFORMERS_AVAILABLE)

if _NEED_STUB:
    # ========== è½»é‡ Stub æ¨¡å¼ ==========
    # åœ¨ CI / æ— ç½‘ç»œ / å•å…ƒæµ‹è¯•åœºæ™¯ä¸‹ï¼Œæˆ–ç¼ºå¤±æ ¸å¿ƒä¾èµ–æ—¶ï¼Œä½¿ç”¨å ä½æ¨¡å‹é¿å…å¯¼å…¥é”™è¯¯
    class HFChatLLM:  # type: ignore
        def __init__(self, base_model: str, lora_dir: Optional[str] = None, max_new_tokens: int = 512, **_):
            self.base_model = base_model
            self.lora_dir = lora_dir or ""
            self.max_new_tokens = max_new_tokens
            self.model_name = "student-stub"
        def _format_prompt(self, obj: Union[Dict, str]) -> str:
            if isinstance(obj, dict):
                return "\n".join(f"{k}: {v}" for k, v in obj.items())
            return str(obj)
        def invoke(self, prompt: Union[Dict, str]) -> str:
            text = self._format_prompt(prompt)
            # ç®€åŒ–è¾“å‡ºï¼Œä»…æˆªæ–­ + æ ‡è®°
            head = text[: min(200, len(text))]
            return f"[STUB_GENERATION]\n{text[:120]}..."
        def __call__(self, prompt: Union[Dict, str]):  # ä½¿å…¶å¯è°ƒç”¨
            return self.invoke(prompt)
        def __or__(self, other):  # ç®€å•é“¾å¼å…¼å®¹ï¼šç›´æ¥è¿”å›ä¸‹æ¸¸å¯¹è±¡
            return other
else:
    class HFChatLLM:
        """HF + å¯é€‰ LoRA èŠå¤©å¼å­¦ç”Ÿæ¨¡å‹å°è£… (.invoke)
        æ”¯æŒæœ¬åœ° Qwen/Qwen1.5-1.8B-Chat åŠ LoRA é€‚é…å™¨åŠ è½½ã€‚
        ä¾èµ–: torch + transformers (+ peft å¯é€‰, + bitsandbytes å¯é€‰ç”¨äº 4bit æ¨ç†)
        """

        def __init__(
            self,
            base_model: str,
            lora_dir: Optional[str] = None,
            max_new_tokens: int = 512,
            device: Optional[str] = None,
            torch_dtype: Optional[str] = None,
            device_map: Optional[str] = None,
            load_in_4bit: bool = False,
        ) -> None:
            self.base_model = base_model
            self.lora_dir = lora_dir or ""
            # ä¸ºäº†æ§åˆ¶æ˜¾å­˜å ç”¨ï¼Œé™åˆ¶å•æ¬¡ç”Ÿæˆé•¿åº¦
            self.max_new_tokens = min(max_new_tokens, 512)
            self.model_name = base_model

            # æ˜¯å¦å¯ç”¨ 4bit é‡åŒ–ï¼ˆä»…åœ¨ GPU + bitsandbytes å¯ç”¨æ—¶ç”Ÿæ•ˆï¼‰
            self.load_in_4bit = bool(load_in_4bit and _BNB_AVAILABLE and torch and torch.cuda.is_available())
            if load_in_4bit and not self.load_in_4bit:
                print("âš ï¸ Requested load_in_4bit=True but bitsandbytes / CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŠç²¾åº¦æ¨¡å¼ã€‚")

            if device is None:
                # é»˜è®¤ä¼˜å…ˆç”¨ CUDAï¼›å¦‚æœä½ æƒ³æ›´çœæ˜¾å­˜ï¼Œå¯ä»¥åœ¨å¤–éƒ¨æ˜¾å¼ä¼  device="cuda" ä¸”é…åˆæ›´å°çš„æ¨¡å‹ / é‡åŒ–
                device = "cuda" if (torch and getattr(torch, 'cuda', None) and torch.cuda.is_available()) else "cpu"
            self.device = torch.device(device) if torch else device

            # dtype é€‰æ‹©ï¼šé»˜è®¤åœ¨ GPU ä¸Šä¼˜å…ˆç”¨ bfloat16/float16 ä»¥èŠ‚çœæ˜¾å­˜
            dtype = None
            if torch and torch_dtype:
                try:
                    dtype = getattr(torch, torch_dtype)
                except Exception:
                    dtype = None
            elif torch and device == "cuda" and not self.load_in_4bit:
                # å¦‚æœç”¨æˆ·æ²¡æ˜¾å¼æŒ‡å®š dtypeï¼Œä¸”æœªå¼€å¯ 4bitï¼Œåˆ™åœ¨ CUDA ä¸Šå°½é‡ç”¨åŠç²¾åº¦
                if getattr(torch.cuda, "is_bf16_supported", lambda: False)():
                    dtype = torch.bfloat16
                else:
                    dtype = torch.float16

            self.tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            load_kwargs = {"trust_remote_code": True}

            # 4bit é‡åŒ–åŠ è½½ä¼˜å…ˆï¼Œå…¶æ¬¡æ˜¯åŠç²¾åº¦
            if self.load_in_4bit:
                load_kwargs.update({
                    "load_in_4bit": True,
                    "device_map": device_map or "auto",
                })
                # 4bit æ¨¡å¼ä¸‹ç”± HF ç®¡ç†è®¾å¤‡åˆ†é…ï¼Œä¸å†æ‰‹åŠ¨ model.to(device)
            else:
                if dtype is not None:
                    load_kwargs["torch_dtype"] = dtype
                if device_map:
                    load_kwargs["device_map"] = device_map

            print(f"ğŸ”„ Loading base model: {base_model}...")
            base = AutoModelForCausalLM.from_pretrained(base_model, **load_kwargs)

            if self.lora_dir and os.path.exists(self.lora_dir) and _PEFT_AVAILABLE:
                try:
                    print(f"ğŸ”„ Loading LoRA adapter: {self.lora_dir}...")
                    self.model = PeftModel.from_pretrained(base, self.lora_dir)
                    print(f"âœ… LoRA adapters loaded successfully.")
                except Exception as e:
                    print(f"âš ï¸ Failed to load LoRA adapters from {self.lora_dir}: {e}. Using base model.")
                    self.model = base
            else:
                if self.lora_dir and not os.path.exists(self.lora_dir):
                    print(f"âš ï¸ LoRA dir not found: {self.lora_dir}. Using base model.")
                if self.lora_dir and not _PEFT_AVAILABLE:
                    print("âš ï¸ peft not available; cannot load LoRA. Using base model.")
                self.model = base

            if not self.load_in_4bit:
                # ä»…åœ¨é 4bit æ¨¡å¼ä¸‹æ‰‹åŠ¨æ¬è¿åˆ°æŒ‡å®šè®¾å¤‡ï¼›4bit + device_map=auto ç”± HF ç®¡ç†
                try:
                    self.model.to(self.device)
                except Exception:
                    pass
            self.model.eval()

        def invoke(self, prompt: Union[Dict, str]) -> str:
            """æ‰§è¡Œä¸€æ¬¡æ¨ç†è°ƒç”¨ï¼Œä½¿ç”¨ Chat æ¨¡æ¿ä»¥ç¡®ä¿æŒ‡ä»¤éµå¾ªã€‚"""
            
            # 1. è§£æè¾“å…¥å†…å®¹
            content = ""
            if isinstance(prompt, dict):
                content = "\n".join(f"{k}: {v}" for k, v in prompt.items())
            else:
                content = str(prompt)

            # 2. æ„å»ºç¬¦åˆ Chat æ¨¡å‹çš„å¯¹è¯å†å²
            # å¢åŠ  System Prompt å¼ºåˆ¶è§„å®šè¾“å‡ºæ ¼å¼ï¼Œé˜²æ­¢æå–å¤±è´¥
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯å†™ä½œä¼˜åŒ–åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆæ ¼å¼è¦æ±‚ï¼Œè¾“å‡º **ä¼˜åŒ–ç‰ˆæœ¬ï¼š** å’Œ **ä¿®æ”¹è¯´æ˜ï¼š**ã€‚"},
                {"role": "user", "content": content}
            ]

            # 3. ä½¿ç”¨ Tokenizer çš„ apply_chat_template è¿›è¡Œæ ¼å¼åŒ–
            # è¿™ä¼šæ·»åŠ  <|im_start|>system...<|im_start|>user ç­‰ç‰¹æ®Š token
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.tokenizer(text, return_tensors="pt")
            
            if torch:
                try:
                    # åœ¨ 4bit + device_map æ¨¡å¼ä¸‹ï¼Œinputs ä»ç„¶éœ€è¦æ”¾åˆ°ä¸»è®¾å¤‡æˆ–ç›¸åº” CUDA
                    inputs = inputs.to(self.device) if not self.load_in_4bit else inputs.to("cuda" if torch.cuda.is_available() else self.device)
                except Exception:
                    pass
                
                with torch.no_grad():
                    output_ids = self.model.generate(
                        **inputs,
                        max_new_tokens=self.max_new_tokens,
                        do_sample=True,      # å¼€å¯é‡‡æ ·ä»¥è·å¾—æ›´è‡ªç„¶çš„æ”¹å†™
                        temperature=0.3,     # ä½æ¸©åº¦ä¿è¯å­¦æœ¯ä¸¥è°¨æ€§
                        top_p=0.9,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.pad_token_id,
                    )
                # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
                return self.tokenizer.decode(gen_ids, skip_special_tokens=True)
            else:
                return "[Torch unavailable: stubbed generation]"

        def __call__(self, prompt: Union[Dict, str]):  # ä¾› LangChain Runnable æ£€æµ‹
            return self.invoke(prompt)

        def __or__(self, other):  # ä¿æŒä¸ PromptTemplate | LLM | Parser é“¾å¼å…¼å®¹
            return other